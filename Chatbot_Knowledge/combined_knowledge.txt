Data Access Information (February 21, 2025)

Kepler Objects of Interest (KOI)
- This CSV file can be downloaded by going to https://exoplanetarchive.ipac.caltech.edu/docs/data.html
  and clicking the KOI Table (Cumulative list) under the Kepler section.
- After clicking, you will be transferred to a table that can be downloaded through the 
  "Download Table" button.
- Make sure to select options to include all columns, all rows, and save as a CSV.
- To use the scripts for this project, name it "KOI_cumulative.csv".
- This table will have contextual data for each transit such as its corresponding star, disposition, etc.
- The CSV file can be directly scanned into the Pandas library as a data frame.

KOI Light Curves
- For the stars that appear in the KOI table, their light curves can be downloaded through https://exoplanetarchive.ipac.caltech.edu/bulk_data_download/
- Under Kepler Time Series Scripts, download the Kepler KOI dataset which is a file named "Kepler_KOI_wget.bat"
- This bat file has a wget command on each line with two file formats for light curve data. Only ".tbl" files are of interest.
- wget is a command-line utility that can download an internet file and for installation refer to https://irsa.ipac.caltech.edu/docs/batch_download_help.html
- Note that these commands have single quotations which may need to be converted to double quotations for Windows
- As of milestone 1, the "data_collection.ipynb" included takes care of this quotation conversion, only downloads ".tbl" extensions, and selects features
  that are relevant to the project and stores them in an SQLite database titled "light_curves.db" with the table titled "LightCurve".
- To replicate the milestone 1 process, the notebook should be manually stopped after downloading the first 2680 stars.
- The download process could take hours or days depending on the system used.
- This database has star ID, time, flux, etc. which is closer to the raw data collected by Kepler.

Processed Data
- "KOI_cumulative_cleaned.csv" is the result of "Scripts/data_processing.ipynb" which preprocesses the KOI CSV file.
- "transits.csv" was generated by "Scripts/training.ipynb" and holds the extracted information from the light curves SQL database that was used for training the models.

This file is the content of "data_collection.ipynb"

Markdown Cell:
## Markdown Content:
# Data Collection

Markdown Cell:
## Markdown Content:
## Creating Database for Light Curves

Code Cell:
```python
import sqlite3

# Create database
conn = sqlite3.connect('light_curves.db')
cursor = conn.cursor()

# Create LightCurve table
cursor.execute(
    """
    CREATE TABLE IF NOT EXISTS LightCurve (
        KEP_ID INTEGER,
        TIME REAL,
        TIMECORR REAL,
        PDCSAP_FLUX REAL,
        PDCSAP_FLUX_ERR REAL,
        SAP_QUALITY INTEGER,
        PRIMARY KEY (KEP_ID, TIME)
    );
    """
    )
```

Code Cell:
```python
import re

def add_lightcurve_to_database(star_id, light_curve_path, cursor):

    """ Takes a .tbl and adds its light curve data to the sql database """

    # Read light curve data
    file_path = light_curve_path
    col_names = ['KEP_ID', 'TIME', 'TIMECORR', 'PDCSAP_FLUX', 'PDCSAP_FLUX_ERR', 'SAP_QUALITY']
    num_headers_found = 0

    with open(light_curve_path, 'r') as file:

        # Read file line by line
        for line in file:

            text = line.strip()

            # After the 4 headers are found, the actual data can be recorded
            if num_headers_found == 4:

                data = re.split(r'\s+', text)

                KEP_ID = star_id

                try:
                    TIME = float(data[0])
                except Exception:
                    TIME = f'NULL'

                try:
                    TIMECORR = float(data[2])
                except Exception:
                    TIMECORR = f'NULL'

                try:
                    PDCSAP_FLUX = float(data[8])
                except Exception:
                    PDCSAP_FLUX = f'NULL'

                try:
                    PDCSAP_FLUX_ERR = float(data[9])
                except Exception:
                    PDCSAP_FLUX_ERR = f'NULL'
                
                try:
                    SAP_QUALITY = int(data[10])
                except Exception:
                    SAP_QUALITY = f'NULL'

                cursor.execute(
                    f"""
                    INSERT INTO LightCurve (KEP_ID, TIME, TIMECORR, PDCSAP_FLUX, PDCSAP_FLUX_ERR, SAP_QUALITY) VALUES ({KEP_ID}, {TIME}, {TIMECORR}, {PDCSAP_FLUX}, {PDCSAP_FLUX_ERR}, {SAP_QUALITY});
                    """
                    )
            
            # 4 lines start with '|' before the actual data so keep count
            if len(text) > 0 and text[0] == '|':
                num_headers_found += 1
```

Code Cell:
```python
import pandas as pd
import subprocess
import os

# Load in data for candidates
KOI_df = pd.read_csv('KOI_cumulative.csv', comment='#')

# Get the light curves for the stars
star_ids = KOI_df['kepid'].unique()
download_file_path = 'D:\Exoplanet_Project\light_curves\downloads\download_time_series.txt'
download_dir = 'D:\Exoplanet_Project\light_curves\downloads'

with open(download_file_path, 'r') as file:

    for i, line in enumerate(file):

        # Get basic information from the line
        data_file_name = line.split("'")[1]
        extension = data_file_name.split('.')[1]
        star_id = int(data_file_name.split('-')[0].replace('kplr', ''))
        cmd = line.replace("'", '"')

        # Skip if it's not a .tbl
        if extension != 'tbl':
            continue

        # Download light curve
        process = subprocess.run(cmd, cwd=download_dir, shell=True)
        if process.returncode == 1:
            print(f'Line Index {i} failed to download')
        else:
            print(f'Line Index {i} successfully downloaded')

        # Add light curve to data base
        light_curve_path = download_dir + f'/{data_file_name}'
        add_lightcurve_to_database(star_id, light_curve_path, cursor)

        # Delete light curve
        if os.path.exists(light_curve_path):
            os.remove(light_curve_path)        
```

Code Cell:
```python
# Commit and close database
conn.commit()
conn.close()
```



This file is the content of "data_processing.ipynb"

Markdown Cell:
## Markdown Content:
# Data Preprocessing and Exploration

Markdown Cell:
## Markdown Content:
# Candidate Planets

Markdown Cell:
## Markdown Content:
### Load Data

Code Cell:
```python
import pandas as pd
import sqlite3

# Load in data for candidates
pd.set_option('display.max_columns', None)
KOI_df = pd.read_csv('KOI_cumulative.csv', comment='#')

# Find kepids in light curve data
conn = sqlite3.connect('light_curves.db')
cursor = conn.cursor()
cursor.execute('PRAGMA cache_size = 1000000')

query = """
    SELECT KEP_ID AS kepid
    FROM LightCurve 
    GROUP BY KEP_ID;
    """

kepids_df = pd.read_sql(query, conn)
```

Code Cell:
```python
# Filter candidates to only include candidates there is light curve data for

available_ids = kepids_df['kepid'].unique()
indices = KOI_df.index
    
for i in indices:

    if KOI_df.loc[i, 'kepid'] not in available_ids:

        KOI_df = KOI_df.drop(index=i)

display(KOI_df)
```

Markdown Cell:
## Markdown Content:
### Acquire Preliminary Understanding

Code Cell:
```python
# Verify data types
print('Data Types')
print('------------------')
for col in KOI_df.columns:
    print(f'{col}: {KOI_df[col].dtype}')
```

Markdown Cell:
## Markdown Content:
Some variables (mainly categorical ones) could benefit from a data type conversion. For example, "koi_disposition" is better suited as a categorical type instead of a string/object. This is noted and will be acted upon after the columns are truncated so that greater attention to detail can be made for the remaining subset of columns. It is not too important to convert object to category since it will be processed equally (disregarding the memory/speed efficiency). However, it is important to convert numerical types that should be category type so that it is not processed as a standard numerical type.

Code Cell:
```python
# Convert appropriate numerical types to category type (flags and ids)
for col in KOI_df.columns:
    if col.find('koi_fp') != -1:
        KOI_df[col] = KOI_df[col].astype('category')

KOI_df['kepid'] = KOI_df['kepid'].astype('category')
KOI_df['rowid'] = KOI_df['kepid'].astype('category')
```

Code Cell:
```python
import numpy as np

# Description for numeric attributes
print('Numeric: Describe Data')
display(KOI_df.describe())

print('Numeric: Relative Standard Deviations')
print('-------------------------------')
for col in KOI_df.select_dtypes(include='number'):
    rel_std = np.abs(KOI_df[col].std()/KOI_df[col].mean())
    print(f'{col}: {rel_std:.2f}')
```

Markdown Cell:
## Markdown Content:
A few columns like "koi_model_dof" are fully null and can be discarded (similarly with constant columns). Asides from that, most features have few null values and can likely be imputed from the non-null values. Judging by the relative standard deviations and mean values, there is a range of sparsity and several features are on different scales so the data would likely benefit from scaling.

Code Cell:
```python
# Description for categorical attributes
print('Categorical: Describe Data')
print('----------------------------')
for col in KOI_df.select_dtypes(exclude='number'):

    print()

    # Print feature name
    print(f'Feature: {col}')

    # Print number of null entries
    num_null = KOI_df[col].isna().sum()
    print(f'Number of Null Values: {num_null}')

    # Print data type
    type = KOI_df[col].dtype
    print(f'Type: {type}')

    # Print number of unique entries
    nunique = KOI_df[col].nunique()
    print(f'Number of Unique Values: {nunique}')

    # Print up to 5 most frequent unique entries and their number of occurrences
    max_unique = 5
    most_frequent = KOI_df[col].value_counts().head(max_unique)
    print(f'Top {max_unique} Most Frequent Values:')

    for i, (value, frequency) in enumerate(most_frequent.items()):

        print(f'\t{i+1}. {value}: {frequency} occurrences')

        if i == max_unique-1:
            break
```

Markdown Cell:
## Markdown Content:
There are several features with one unique value which would provide no information. There are also features with values that require additional context (e.g. "koi_datalink_dvs" is a pdf file which is only useful if one goes to its website and reads its text but this would be difficult without something like an LLM). "rowid" seems to be a counter so it can later be discarded.

Markdown Cell:
## Markdown Content:
### Drop Columns That Provide No Information

Markdown Cell:
## Markdown Content:
Columns with fully null or constant values provide no predictive information and cannot be imputed with their own data so they will be dropped.

Code Cell:
```python
# Drop fully null or constant features
null_or_constant_cols = []

for col in KOI_df.columns:

    if KOI_df[col].nunique() <= 1:

        null_or_constant_cols.append(col)

KOI_df = KOI_df.drop(columns=null_or_constant_cols)
print(f"Dropped due to being fully null or constant: {null_or_constant_cols}")
```

Markdown Cell:
## Markdown Content:
### Look for Duplicates

Code Cell:
```python
# Print number of duplicate rows
print(f'Number of Duplicate Rows: {KOI_df.duplicated().sum()}')
```

Markdown Cell:
## Markdown Content:
### Analyze Correlation and Chi-Squared

Code Cell:
```python
import matplotlib.pyplot as plt
import seaborn as sns

# Show high correlations (absolute value is greater than or equal to threshold)
# Exclude correlations of a variable with itself
threshold = 0.8
corr = KOI_df.corr(method='pearson', numeric_only=True)
high_corr = {}

for row in corr.index:
    for col in corr.columns:

        if col == row:
            break

        pearson = corr.loc[row, col]

        if np.abs(pearson) >= threshold:
            if row not in high_corr:
                high_corr[row] = [col]
            else:
                high_corr[row].append(col)

print(f'High Pearson Correlations (>= {threshold})')
print('------------------------------------------')
col_length = 22
print(f'{"Feature 1":<{col_length}} {"Feature 2":<{col_length}} {"Pearson":<{col_length}}')
print(f'{"-------------------":<{col_length}} {"-------------------":<{col_length}} {"---------":<{col_length}}')

for row in high_corr:
    for col in high_corr[row]:
        print(f'{row:<{col_length}} {col:<{col_length}} {corr.loc[row, col]:<{col_length}.2f}')
```

Markdown Cell:
## Markdown Content:
There are about 3 predominant categories of high correlations: features related to star/planet characteristics, equipment, and errors. In the future, this could be used to condense the dataset since key characteristics of a star would likely speak for all other characteristics derived from them and the same is true for data related to the equipment. Some upper limit errors are correlated to lower limit errors which is redundant information. Interestingly, the value of some variables are correlated to the errors which indicates there are features that have increased/decreased error depending on their value.

Code Cell:
```python
from scipy.stats import chi2_contingency

# Compute pearson chi-squared (allow up to 20% of expected frequencies to be less than 5, otherwise mark p as 2)
# Scipy documentation says this test is unreliable for frequencies less than 5
categorical_names = KOI_df.select_dtypes(exclude='number').columns
chi2_dict = {}

for i in range(len(categorical_names)):
    name = categorical_names[i]
    chi2_dict[name] = [np.nan for _ in range(len(categorical_names))]

p_matrix = pd.DataFrame(chi2_dict, index=categorical_names, columns=categorical_names)

for row in categorical_names:
    for col in categorical_names:

        crosstab = pd.crosstab(KOI_df[row], KOI_df[col])
        chi2, p, dof, expected_freq = chi2_contingency(crosstab)
        threshold = 0.2
        freq_ratio_less_than_5 = expected_freq[expected_freq<5].size/float(expected_freq.size)

        if freq_ratio_less_than_5 > threshold:
            p_matrix.loc[row, col] = 2
        else:
            p_matrix.loc[row, col] = round(p, 2)


```

Code Cell:
```python
# Plot p-values from chi-squared test
plt.figure(figsize=(12,5))
sns.heatmap(p_matrix, annot=True, cmap='coolwarm')
plt.title('P-Values from Chi-Squared Test for Non-Numerical Features', pad=15)
plt.annotate(text=r'* Frequencies less than 5 decrease reliability, so if over 20% of expected frequencies for a pair were less than 5, the p-value was marked as 2', xy=(-100,-130), xycoords='axes points')
plt.show()
```

Markdown Cell:
## Markdown Content:
### Visualizations for Target and Features of Current Interest

Markdown Cell:
## Markdown Content:
The target variable will be "koi_disposition" so the analysis will have greater emphasis on understanding relationships to this variable.

Code Cell:
```python
# Distribution of categories in "koi_disposition" (target)
data = KOI_df.groupby('koi_disposition')['koi_score'].agg(['mean', 'median'])
width = 0.8
ax = data.plot.bar(width=width, figsize=(9,6), edgecolor='black')
ax.grid(linestyle='--')
ax.set_axisbelow(True)
plt.title('Level of Confidence in Disposition', pad=15, fontsize=14)
plt.xlabel('koi_disposition', labelpad=15, fontsize=12)
plt.ylabel('koi_score (0 to 1)', labelpad=15, fontsize=12)
plt.xticks(fontsize=11)
plt.yticks(fontsize=11)
plt.ylim([0,1.2])

for i, disposition in enumerate(data.index):
    mean = data.loc[disposition, 'mean']
    median = data.loc[disposition, 'median']
    ax.annotate(text=f'{mean:.2f}', xy=(i-width/4, mean+0.02), ha='center', fontsize=12)
    ax.annotate(text=f'{median:.2f}', xy=(i+width/4, median+0.02), ha='center', fontsize=12)

plt.legend(fontsize=11)
plt.show()
```

Code Cell:
```python
# Histogram of disposition score
KOI_df['koi_score'].plot.hist(figsize=(10,8), bins=10, edgecolor='black')
plt.title('Distribution of Disposition Confidence', pad=15, fontsize=14)
plt.xlabel('koi_score (0 to 1)', labelpad=15, fontsize=12)
plt.ylabel('Number of Scores', labelpad=15, fontsize=12)
xticks = [i for i in np.arange(0,1.1,0.1)]
yticks = [i for i in range(0, 1600, 100)]
plt.xticks(ticks=xticks, fontsize=12)
plt.yticks(ticks=yticks, fontsize=12)
plt.grid(linestyle='--')
plt.gca().set_axisbelow(True)
plt.ylim([0,1400])

plt.show()
```

Code Cell:
```python
import os

# Save hist and box for numerical types, bar for categorical types (with 10 or fewer unique categories)
cat_fts = KOI_df.select_dtypes(exclude='number').columns
num_fts = KOI_df.select_dtypes(include='number').columns

if not os.path.exists('plots'):
    os.mkdir('plots')

if not os.path.exists('plots/distributions'):
    os.mkdir('plots/distributions')

for feature in cat_fts:

    if KOI_df[feature].nunique() > 10:
        continue

    print(f'Saving figures for {feature}')
    plt.clf()
    ax = KOI_df.groupby(feature)[feature].count().plot.bar(edgecolor='black')
    plt.xlabel(feature)
    plt.ylabel('Number of Occurrences')
    plt.tight_layout()
    plt.savefig(f'plots/distributions/{feature}-bar.png', dpi=300)

for feature in num_fts:
    print(f'Saving figures for {feature}')
    plt.clf()
    ax = KOI_df[feature].plot.hist(edgecolor='black')
    plt.xlabel(feature)
    plt.ylabel('Number of Occurrences')
    plt.tight_layout()
    plt.savefig(f'plots/distributions/{feature}-hist.png', dpi=300)
    plt.clf()
    ax = KOI_df[feature].plot.box()
    plt.ylabel(f'Value of {feature}')
    plt.tight_layout()
    plt.savefig(f'plots/distributions/{feature}-box.png', dpi=300)
```

Code Cell:
```python
# Make scatter plots to see relationship between detection direction and star id
plt.scatter(KOI_df['kepid'], KOI_df['dec'], s=5)
plt.title('Light Detection Direction: Declination', pad=15)
plt.xlabel('kepid', labelpad=15)
plt.ylabel('dec (decimal degrees)', labelpad=15)
plt.tight_layout()
plt.show()

plt.scatter(KOI_df['kepid'], KOI_df['ra'], s=5)
plt.title('Light Detection Direction: Right Ascension', pad=15)
plt.xlabel('kepid', labelpad=15)
plt.ylabel('ra (decimal degrees)', labelpad=15)
plt.tight_layout()
plt.show()
```

Code Cell:
```python
# Plot distribution of false positive flags
flags = ['koi_fpflag_nt', 'koi_fpflag_ss', 'koi_fpflag_co', 'koi_fpflag_ec']
counts = []

for flag in flags:
    count = KOI_df[flag].astype(int).sum()
    counts.append(count)

plt.figure(figsize=(10,6))
plt.bar(flags, counts, edgecolor='black')
plt.title('False Positive Flags', pad=15, fontsize=14)
plt.xlabel('Flag Types', labelpad=15, fontsize=12)
plt.ylabel('Number of Occurrences', labelpad=15, fontsize=12)
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)

for i, count in enumerate(counts):
    plt.annotate(text=f'{count}', xy=(i, count+6), ha='center', fontsize=12)

plt.grid(linestyle='--')
plt.gca().set_axisbelow(True)
plt.tight_layout()
plt.show()
```

Markdown Cell:
## Markdown Content:
### Detect Outliers

Code Cell:
```python
from scipy.stats import zscore

# This function was provided in the Data Wrangling Demo by Grant & Castro (CAP5771, University of Florida, 2025)
def detect_outliers_democratic(df, min_agreement = 2):
    df = df.copy()  
    num_cols = df.select_dtypes(include=['number']).columns  
    outlier_summary = {}

    print(f'Numerical Outliers: Agreement of {min_agreement} out of 3 Methods')
    print('----------------------------------------------------------------------')

    for col in num_cols:
        values = df[col].dropna()  
        
        Q1 = np.percentile(values, 25)
        Q3 = np.percentile(values, 75)
        IQR = Q3 - Q1
        iqr_lower = Q1 - 1.5 * IQR
        iqr_upper = Q3 + 1.5 * IQR
        iqr_outliers = values[(values < iqr_lower) | (values > iqr_upper)].index

        z_scores = zscore(values)
        z_outliers = values[np.abs(z_scores) > 3].index

        median = np.median(values)
        mad = np.median(np.abs(values - median))
        mad_threshold = 3 * mad
        mad_outliers = values[np.abs(values - median) > mad_threshold].index

        all_outliers = list(iqr_outliers) + list(z_outliers) + list(mad_outliers)
        outlier_counts = pd.Series(all_outliers).value_counts()
        final_outliers = outlier_counts[outlier_counts >= min_agreement].index.tolist()

        if final_outliers:
            outlier_summary[col] = final_outliers

    if outlier_summary:
        for col, indices in outlier_summary.items():
            print(f"{col}: {len(indices)} outliers detected at {indices}")

    return outlier_summary
```

Code Cell:
```python
# Find numerical outliers
outliers_2agree = detect_outliers_democratic(KOI_df, 2)
print()
outliers_3agree = detect_outliers_democratic(KOI_df, 3)
```

Code Cell:
```python
# Save scatter plots of kepid compared to each feature (with outliers marked, 2/3 outlier detection methods agreed)

if not os.path.exists('plots'):
    os.mkdir('plots')

if not os.path.exists('plots/outliers'):
    os.mkdir('plots/outliers')

for feature in KOI_df.select_dtypes(include='number').columns:

    print(f'Saving figures for {feature} (2/3 agreement)')

    if feature not in outliers_2agree:
        is_outlier = [False for _ in range(KOI_df.shape[0])]
    else:
        is_outlier = KOI_df.index.isin(outliers_2agree[feature])

    colors = []

    for i in range(len(is_outlier)):

        if is_outlier[i] == True:
            color = 'red'
        else:
            color= 'green'

        colors.append(color)

    plt.clf()
    plt.scatter(KOI_df['kepid'], KOI_df[feature], c=colors, s=5)
    plt.scatter([], [], c='red', label='Outlier')
    plt.scatter([], [], c='green', label='Not Outlier')
    plt.xlabel('kepid')
    plt.ylabel(f'{feature}')
    plt.legend()
    plt.tight_layout()
    plt.savefig(f'plots/outliers/{feature}-2-out-of-3-agreement.png', dpi=300)

# Save scatter plots of kepid compared to each feature (with outliers marked, 3/3 outlier detection methods agreed)

for feature in KOI_df.select_dtypes(include='number').columns:

    if feature not in outliers_3agree:
        print(f'{feature} has 0 outliers so nothing to save (3/3 agreement)')
        continue

    print(f'Saving figures for {feature} (3/3 agreement)')

    if feature not in outliers_3agree:
        is_outlier = [False for _ in range(KOI_df.shape[0])]
    else:
        is_outlier = KOI_df.index.isin(outliers_3agree[feature])

    colors = []

    for i in range(len(is_outlier)):

        if is_outlier[i] == True:
            color = 'red'
        else:
            color= 'green'

        colors.append(color)

    plt.clf()
    plt.scatter(KOI_df['kepid'], KOI_df[feature], c=colors, s=5)
    plt.scatter([], [], c='red', label='Outlier')
    plt.scatter([], [], c='green', label='Not Outlier')
    plt.xlabel('kepid')
    plt.ylabel(f'{feature}')
    plt.legend()
    plt.tight_layout()
    plt.savefig(f'plots/outliers/{feature}-3-out-of-3-agreement.png', dpi=300)
```

Markdown Cell:
## Markdown Content:
Visually, unanimous agreement (3/3) seems to be a better outlier detection. 2/3 agreement considers a large portion of the data to be outliers which would likely lead to insufficient amount of data to train off. Going forward, outliers will require unanimous agreement.

Markdown Cell:
## Markdown Content:
### Impute

Markdown Cell:
## Markdown Content:
The strategy will be to impute missing valus with median values for numerical values and the most frequent category for categorical variables. Outliers will not be included in the statistics used for imputing. "koi_disposition" is the target and will not be imputed. "koi_pdisposition" is closely related to the target so the same will follow. For categorical variables, if more than 50% of the non-null values are the same then it would be imputed. This prevents imputing for columns like "kepler_name" (planet name) which would not make sense to guess by frequency alone.

Code Cell:
```python
# Select numerical and categorical columns
num_cols = KOI_df.select_dtypes(include='number').columns
cat_cols = KOI_df.select_dtypes(exclude='number').drop(columns=['koi_disposition', 'koi_pdisposition']).columns

print('Numerical Imputations')
print('------------------------')

# Impute numerical columns with median

for col in num_cols:

    if col not in outliers_3agree:
        not_outlier = [True for _ in range(KOI_df.shape[0])]
    else:
        not_outlier = ~KOI_df.index.isin(outliers_3agree[feature])

    median = KOI_df[not_outlier][col].median()
    num_nulls = KOI_df[col].isna().sum()

    if num_nulls > 0:
        KOI_df[col] = KOI_df[col].fillna(median)
        print(f'Imputed {num_nulls} nulls of {col} with {median}')

print()
print('Categorical Imputations')
print('------------------------')

# Impute categorical columns with median
for col in cat_cols:

    most_frequent = KOI_df[col].value_counts().index[0]
    num_occurrences = KOI_df[col].value_counts().iloc[0]
    total_values = KOI_df[~KOI_df[col].isna()].shape[0]
    num_nulls = KOI_df[col].isna().sum()
    threshold = 0.5

    if num_nulls > 0 and num_occurrences/total_values > threshold:
        KOI_df[col] = KOI_df[col].fillna(most_frequent)
        print(f'Imputed {num_nulls} nulls of {col} with {most_frequent}')
```

Markdown Cell:
## Markdown Content:
### Quality Check

Markdown Cell:
## Markdown Content:
At this point, there is a better feel for which variables are important. Here, these will be dealt with by dropping irrelvant features and rows. There is reservation to drop rows because that would be an entire star's data that is erased and a lot of corresponding light curve data would go to waste. When appropriate, a bad feature is preferred to be dropped rather than rows that it has nulls for.

Code Cell:
```python
# Show columns that still have null values
print('Columns with nulls remaining')
print('---------------------------------')

i = 1
for col in KOI_df.columns:
    num_nulls = KOI_df[col].isna().sum()
    if num_nulls > 0:
        print(f'{i}. {col} still has {num_nulls} nulls')
        print(f'Number of unique entries: {KOI_df[col].nunique()}')
        print('Sample of Up to 5 Values:')
        display(KOI_df[col].head())
        print()
        i += 1
```

Markdown Cell:
## Markdown Content:
All the remaining columns with null values are non-numerical and suffer from relatively high cardinality. It would be difficult to make meaningful categories. Based off previous observations, there are many other features with potential for better predictive power so these can safely be dropped.

Code Cell:
```python
# Drop remaining columns with nulls
for col in KOI_df.columns:
    num_nulls = KOI_df[col].isna().sum()
    if num_nulls > 0:
        KOI_df = KOI_df.drop(columns=col)
        print(f'Dropped {col}')
```

Code Cell:
```python
# Show non-numerical columns
print('Non-Numerical Columns')
print('---------------------------------')

i = 1
for col in KOI_df.select_dtypes(exclude='number').columns:
    print(f'Number of unique entries: {KOI_df[col].nunique()}')
    print('Sample of Up to 5 Values:')
    display(KOI_df[col].head())
    print()
    i += 1
```

Markdown Cell:
## Markdown Content:
"koi_quarters" and "kepoi_name" have high cardinality and better represent their information as strings. However, the other non-numerical columns can be converted to a categorical type since they are low cardinality and actual categories. "rowid" may also be dropped at this point since it is just a counter and the data frame's index or "kepoi_name" serve as better identifiers. Some floats are also better suited as integers.

Code Cell:
```python
# Convert categorical variables to the categorical data type
for col in KOI_df.select_dtypes(exclude='number').drop(columns=['koi_quarters', 'kepoi_name']):
    KOI_df[col] = KOI_df[col].astype('category')
    print(f'Converted {col} to category data type')

# Drop rowid (counter)
KOI_df = KOI_df.drop(columns='rowid')
print('Dropped rowid')
```

Code Cell:
```python
# Convert int to float
KOI_df['koi_tce_plnt_num'] = KOI_df['koi_tce_plnt_num'].astype(int)
print('Converted koi_tce_plnt_num to int')
```

Markdown Cell:
## Markdown Content:
### Normalize

Markdown Cell:
## Markdown Content:
To prepare for PCA, standard scaler will be used on features (not target).

Code Cell:
```python
# Current data frame
print('KOI Data Before Scaling')
display(KOI_df)
```

Code Cell:
```python
from sklearn.preprocessing import StandardScaler

# Apply standard scaler to numerical features
scaler = StandardScaler()
KOI_scaled = KOI_df.copy()
num_features = KOI_scaled.select_dtypes(include='number').columns
scaler.fit(KOI_scaled[num_features])
KOI_scaled[num_features] = scaler.transform(KOI_scaled[num_features])

print('KOI Data After Scaling')
display(KOI_scaled)
```

Markdown Cell:
## Markdown Content:
### Save Cleaned KOI Data

Code Cell:
```python
KOI_scaled.to_csv('KOI_cumulative_cleaned.csv', index=False)
```

Markdown Cell:
## Markdown Content:
## Light Curves

Markdown Cell:
## Markdown Content:
### Basic Information

Code Cell:
```python
# Connect to database
conn = sqlite3.connect('light_curves.db')
cursor = conn.cursor()
cursor.execute('PRAGMA cache_size = 1000000')

# Display first 5 rows
query = """
    SELECT *
    FROM LightCurve
    LIMIT 5;
    """

df = pd.read_sql(query, conn)
print('First 5 Rows')
print(df)

# Display number of nulls per column
columns = ['KEP_ID', 'TIME', 'TIMECORR', 'PDCSAP_FLUX', 'PDCSAP_FLUX_ERR', 'SAP_QUALITY']

for col in columns:

    query = f"""
        SELECT COUNT(*) AS num_nulls
        FROM LightCurve
        WHERE {col} IS NULL;
        """

    df = pd.read_sql(query, conn)
    print()
    print(f'{col} Nulls')
    print(df)

query = f"""
        SELECT COUNT(*) AS num_nulls
        FROM LightCurve
        WHERE KEP_ID IS NULL OR TIME IS NULL OR TIMECORR IS NULL OR PDCSAP_FLUX IS NULL OR PDCSAP_FLUX_ERR IS NULL OR SAP_QUALITY IS NULL;
        """

df = pd.read_sql(query, conn)
print()
print(f'Number of Complete Null Rows')
print(df)
```

Markdown Cell:
## Markdown Content:
Due to the current size and structure of the light curve data, as well as computational constraints, queries are extremely slow. At this point, it is difficult to acquire summary statistics via SQL queries. However, insights from the KOI dataset are closely related to the light curve data. Star compositions, locations, etc. were already processed via the KOI dataset. Combined with the sample and null counts, there is sufficient understanding of the light curve data for the exploratory phase. More in depth analysis can be done during feature selection since the dimensionality should be reduced in that phase. Removing nulls is not necessary at this time since they provide valuable information. For example, periodic missing times could mean the detectors regularly turn off and this could be important to know, both for personal knowledge and possibly for the model to learn. SAP_QUALITY has no nulls and usually describes why the rest are missing via codes. Also, a model of interest is RNN which can have a masking layer that skips nulls. NASA provides their own documentation which combined with domain knowledge gives the general idea of the summary statistics. Finally, due to these computational constraints, the KOI and light curve data will be "virtually" merged. They will exist in their own locations, but KOI has a star id feature which will be able to identify rows in the light curve data to select. A physical merge will happen right before model training since the data would be significantly reduced by then.



This file is the content of "../Report/Milestone2.pdf"

Identifying Exoplanets from Kepler Light Curves: Milestone 2
Michael Calderin∗
University of Florida CAP5771
(Dated: April 4, 2025)
1. INTRODUCTION ortime-consumingtomeasurewouldnotbebeneficialto
NASA’s pipeline.
Starting in 2009 and continuing for 9.6 years, NASA’s Topresentfindings,aninteractiveconversationalagent
Kepler/K2 missions set out to hunt for planets outside willbeused. Hopefully,thiswillbevisuallyappealingin-
of our solar system [1]. A large part of the identification stead of in a command-line but it will depend on model
process was to record the flux (brightness) from stars in performance and time constraints. A logistic regression,
a small patch of our galaxy and detect when there is a Recurrent Neural Network (RNN), and Random Forest
dip in the flux. The dips are typically signs of a planet Classifier model will be trained. SQLite will be used for
crossing the star. For a planet, these transits are peri- the bulk of the storage, meaning the SQLite3 module.
odic and can be fit to help estimate parameters such as Pandas, NumPy, and SciPy will be used for data manip-
the planet’s size, distance from its star, etc. These fit- ulation, and Matplotlib and Seaborn for visualizations.
tingmodelsalsogivebetterquantificationforthetransit Scikit-learn and TensorFlow will be used for preprocess-
depth (the amount that the flux falls during the transit) ingandbuildingthemodels. Asfortheagent,ChatGPT
and other transit-related features. API, and Rasa are contenders.
However,notalltransitsareplanets. Somestarscome
in pairs and can also have transits. These are known as
eclipsingbinaries. Thereareotherfalsepositivessuchas 3. TIMELINE
interference from the light of other stars. For a transit
to be confirmed as a planet, there is typically a pipeline February 24, 2025 - March 9, 2025
that requires additional observations and can take years.
There are several planetary candidates that to this day The database will be modified to use the star ID
have not been confirmed to be planets. NASA uses and time stamps of a light curve as the primary key
Robovetter,adecisiontree,toautomatetheclassification which will speed up queries. A decision tree classifier
process. It distinguishes between candidates and false will be trained to view its feature importances and help
positives but does not make predictions for true plan- reduce the dimensionality of the data.
ets, and even with automation, the full pipeline can be
long. Detected transits have their classification stored March 10, 2025 - March 16, 2025
in a ”Kepler Objects of Interest” (KOI) table. Some
of these classifications/dispositions are from automated Features will be selected based on previous analy-
processes like Robovetter and others are human-verified. sis. Samples will be split and training/hyper-tuning
These will be used to train classification models in an will begin for the three supervised classification models,
attempt to replicate NASA’s exoplanet pipeline. iteratively evaluating performance metrics.
March 17, 2025 - March 21, 2025
2. OBJECTIVE AND TECH STACK
Final attempts for improvements will be made along
The primary motive will be to classify transits based with an analysis of the strengths, weaknesses, and biases
off the light curves acquired during the Kepler mission of each model. The report and GitHub will be updated.
and additional contextual data. A transit will be clas-
sified as either a confirmed planet or a false positive. March 22, 2025 - March 30, 2025 Using the
Features chosen for each model will vary and be engi- test set, the best models based on previous performance
neered to best suit the model. Generally, the flux and will be run. Insights and limitations will be explored.
times for light curves of stars combined with features
that provide additional context will be used. Ideally, the March 31, 2025 - April 13, 2025 The conver-
features should have strong predictive power but also be sational agent will be researched and explored. The
non-trivial and relatively simple to measure in practice. models might be deployed as an API and use an
Features with strong predictive power that are difficult SQL database to query from for more reliable re-
sponses; the agent will likely be fed the final report
andimportantcodesectionstoanswergeneralquestions.
∗Electronicaddress: michaelcalderin@ufl.edu April 14, 2025 - April 23, 20252
With the exception of a few features, the features
The final presentation and submission will be done. were mainly analyzed and not dropped at this stage.
Those that were dropped were mainly due to being en-
tirely null, constant, or in the interest of preserving the
4. DATA COLLECTION most useful data while having no null values. Basic in-
formation for each feature such as their null count, de-
scriptive statistics, most frequent values, etc. were dis-
All data used is publicly accessible through NASA’s
played. There were no duplicates. Some features that
API.Therearenoexplicitlicensingorusagerestrictions,
obviously needed data types conversions were handled.
especially for the scope of this project. NASA’s exo-
Pearson correlation coefficients were calculated for nu-
planetarchiveprovidesa”CumulativeKeplerObjectsof
merical features and chi-squared for categorical features.
Interest (KOI)” table in the form of a CSV which has
Histograms,frequencybargraphs,andboxplotsforeach
summary information about each star’s transits. This is
feature were saved to a folder. Outliers were detected
where the potential exoplanets’ dispositions are labeled
but not removed at this stage. However, they were ex-
as confirmed, candidate, or false positive. This was di-
cludedforimputationpurposes. Numericalfeatureswere
rectly downloaded through their website [2] and saved
scaled using the scikit-learn standard scaler. Scaling
as KOI cumulative.csv. The light curve data was more
was done at this point to help with feature selection,
complicated to fetch since there is 3 TB worth of light
but each model has its own pipeline that varies and will
curves in their database. There was a section of the
later be discussed. The cleaned KOI data was saved as
archive for bulk downloads that provided a script called
KOI cumulative cleaned.csv. The SQL database of light
Kepler KOI wget.bat [3]. Itcontainsawget commandon
curves originally had slow query times so its exploration
eachlinethatfetcheslightcurvedataforeachstarthatis
was limited and its content was left untouched until the
in the KOI table and would likely amount to more than
feature selection phase; that is when a clearer picture
100 GB when fetched in its entirety. Each star’s light
was drawn for how the content of the database would
curve is its own dataset.
be utilized. For more information, refer to the Jupyter
InaJupyterNotebooktitleddata collection.ipynb,the
Notebook which has markdown cells with details and in-
bat file was processed line by line. Single quotation
sights.
marks had to be converted to double quotations to be
able to run the commands on Windows. The com-
mands were run through Python using its subprocess
module. The data was saved in an SQLite database ti- 6. EXPLORATORY INSIGHTS
tled light curves.db and due to the size of the data, only
relevant features were kept and the process was stopped Duetothelargenumberoffeatures, somekeyinsights
aftercollectingdataforthefirst2680starswhichisabout will be discussed but they are in no means exhaustive.
a quarter of the stars in the KOI table and roughly 20 Tobegin,Pearsoncorrelationsshowedthreepredominant
GB.Sincetransitsaretypicallyperiodic,eachplanetwill areas of high correlations (above 0.8): star/planet char-
havemultipledipsinitscorrespondingstar’slightcurve; acteristics, equipmentinformation, anderrors. Meaning,
theserepetitionswillbeutilizedtoaugmentthedatasize. features related to stellar or planetary data could likely
Withthissample,SQLqueriesarealreadyslow. Interms be reduced to a few key features and the same goes for
of downloading the data, it took two days to fetch these theothertwocategories. Thechi-squaredtestisshownin
stars alone. It would have been ideal to use a random Figure 1. Most dependent relationships are contextually
sample instead of going based on first come first serve, obvious. For example, the disposition and false positive
but due to computational constraints it would take too flags would be related because one of the classes of dis-
long to download a random sample at this time. The position is false positive and the flags are simply a more
potential for bias will be noted and tracked throughout descriptive version of that. This gives strong indication
the project. thatthecategoricalfeaturescouldbecondensed. Despite
this, no features were dropped during the exploratory
phase since that was better suited for the feature selec-
5. DATA CONTENT AND PREPROCESSING tion and engineering phase. The main emphasis during
the exploratory phase was to understand the data and
The data was analyzed in data processing.ipynb. The acquire enough evidence to justify feature selections.
KOICSVwasreadinasaPandasdataframebutfiltered The target variable is ”koi disposition” and it was im-
so that it would only include stars that also appeared portanttounderstanditsdistribution. Itturnsoutthere
in the SQL light curve database. The KOI data had are about 700 candidates, 1000 confirmed planets, and
3164 rows and 141 columns with features such as star 1400 false positives. The imbalance indicates that strat-
ID, transit time, duration, etc. The SQL database had ification might be useful for modeling. However, during
features such as star ID, time of measurement, flux, and feature selection/engineering, multiple transits per star
quality of measurement. There were six columns and were used so this distribution changed and made the
over 200 million rows. number of false positives closely match the number of3
FIG. 1: The p-values after running the chi-squared test on
non-numericalfeatures. P-valuesof2areobviouslynonphys-
icalandindicatethatthechi-squaredwouldbeinaccuratefor
thatpairoffeaturesduetoasmallsizeinexpectedfrequency
whichdecreasesreliabilityaccordingtothescikit-learndocu- FIG. 2: The mean/median level of confidence in each dispo-
mentation. sition is displayed. These scores are generated by a Monte
Carlo technique such that the score’s value is equivalent to
the fraction of iterations where NASA’s automated classifier
(Robovetter) outputs ”CANDIDATE”.
confirmed planets; this will later be revisited.
InFigure2,higherscoresforcandidatesandconfirmed
planets indicate greater confidence in the classification
while for false positives, lower scores indicate greater
confidence. Across the board, there is high confidence
in each disposition. Still, there is a notable imbalance,
specifically for candidates, between median and mode.
Likely, low-confidence outliers are skewing this category.
This could be due to false positives having more obvi-
ouspatternsandconfirmedplanetshavingmorerigorous
processinginthepipeline,whilecandidateshavelesspre-
dictable trends and are somewhat in the middle between
false positives and confirmed.
AsshowninFigure3,thecurrentsamplefromtheKe-
pler light curve data was the outer edges of the field of
view. There are no points from the center. Given that
Kepler only captured a small section of the sky and it
was our own galaxy, the data is innately biased aside
FIG. 3: ”kepid” is a unique identifier for each star and is
from sampling. It still would have been more represen-
plotted against right ascension. The accompanying coordi-
tative of the data at hand to do randomized sampling,
nate to right ascension, declination, varied from roughly 36
butthedataisultimatelybiasedregardlessandcomputa-
to52decimaldegreesandwasalsomissingstarsinthemiddle.
tionalconstraintsprevented”fair”representation. There It provided no new information in terms of sample selection
are also drawbacks to randomized sampling such as not that this right ascension visualization did not encapsulate.
getting enough data from neighboring stars which could
lead to a lack of recognition of light interference type
false positives. The problem at hand has many complex the data as outliers. This is not only inconvenient since
factors at play so optimized sampling would be a study training typically requires as much data as possible, but
of its own. also disconnected from visual insights. Upon inspection,
Figure4showsthatthetypesoffalsepositivesarealso many of the ”outliers” are generally part of the cluster
imbalanced. Due to the size of the data, it is difficult to of data. Unanimous outlier detection seems to be a bet-
find all imbalances but clearly they are present so this ter fit. Thus, values will be considered outliers if there
should be noted. is unanimous agreement. These points will be identi-
In terms of outliers, a democratic method was em- fied as outliers but not discarded. Discarding even one
ployed between z-score, inter-quartile range, and me- outlier would mean a large amount of light curve data
dian absolute deviation for each feature. Generally, us- is thrown out. It would also be strange to remove out-
ing two-thirds agreement was considering too much of liers since transits themselves are rare events compared4
not a machine learning model. Based off this description
and its previous high correlation to the class labels, it
will likely be a strong predictor. In spite of this, part of
theinterestofthisprojectistoreplicateRobovetterand
see if improvements can be made to NASA’s pipeline.
It would be counterproductive to include data generated
by Robovetter, so ”koi pdisposition” will not be used.
Similar logic is used to exclude ”koi score”, which gives
Robovetter’s confidence in its disposition, and the false
positive flags (generated by Robovetter).
Features related to errors, such as the margin of er-
ror for a transit period, were dropped due to their high
collinearity and contribution of noise to the data. These
FIG. 4: This is the distribution of false positive flags. ”nt”
error-based features saturate the feature space which
is not transit-like, ”ss” is stellar eclipse, ”co” is centroid off-
would likely prevent the decision tree classifier from ac-
set (detecting light from a different, nearby star), and ”ec”
curately picking out the most relevant features.
is ephemeris contamination (flux contamination or electrical
crosstalk). The target label, ”koi disposition”, was encoded using
the ordinal encoder from scikit-learn. The false posi-
tive class was marked as 0, candidate was marked as 1,
to the number of data points in a light curve. For more and confirmed was marked as 3. This artificial ordering
specifics on summary statistics, distributions, etc., refer was to imply the ”closeness” that a sample is to being a
to the Jupyter Notebook which is documented step-by- planet. Few categorical features were left and they were
step. relatively low cardinality so they were one-hot encoded;
they also had no implicit ordering to justify an ordinal
encoding.
7. FEATURE ENGINEERING AND SELECTION At this point, the KOI data was clean enough to
train a decision tree classifier that could help with fea-
7.1. Encoding and Data Reduction ture selection. A tree was used because scikit-learn
provides an accessible list of feature importances which
KOI cumulative cleaned.csv wasusedforfeatureselec- represents the importance of the variable in making
tion purposes. The ”kepoi name” feature was a unique its splits/classifications; trees naturally capture non-
identifier for each transit so it had no predictive value linearity and NASA’s pipeline is also rule-based when
and was dropped (along with similar identifiers). Note flagging false positives which is similar to a tree’s behav-
that although it was dropped for feature selection, it ior. Thedatafromthelightcurvedatabasewasnotused
wasstillusefulforidentificationpurposesthroughoutthe here because the emphasis was to reduce dimensionality;
pipeline. Thus, a feature may be discarded for a partic- thelightcurvedatabasehadamuchsmallerfeaturespace
ular task such as feature selection or model training, but so seeing the most relevant features was much more ob-
be reintroduced to provide additional context for perfor- vious. Figure5showsasnippetofcodeusedfortraining
mance. For example, planet size could be irrelevant for the tree. When splitting the data into 80% for train-
such tasks, but it might be interesting to see how model ingand20%fortesting, stratificationbythetargetlabel
performance varies by planet size. was used so that the ratios of the classes were preserved.
”koi quarters” was a binary string where each bit rep- The tree was hyperparameter tuned with K-Fold cross-
resented whether data was collected in that quarter of validation(5folds). Optimizationwasbasedonprecision
the Kepler mission. Although this format is easy for a since the desired behavior of the models was to be confi-
human to read, the models would likely benefit from a dentinitspredictions; discoveringanewplanetisabold
more intuitive form. It was engineered into two forms. claim and it would be disappointing to later find out it
The first was 32 new features, each with a binary digit wasnotatrueplanet. Thebesttreehadthefollowinghy-
thatrepresentedwhetherdatawascollectedinthatquar- perparameters: ”criterion” set to entropy, ”max depth”
ter or not. The second form was one new feature that of 10, ”min samples leaf” of 2, and ”min samples split”
represented the number of quarters that data was col- of 200. For validation, the mean metrics across all folds
lected in. Later, a decision tree classifier was used and were 0.77 for precision, 0.75 for recall, 0.76 for F1, 0.80
itslistoffeatureimportancesshowedthatotherfeatures for accuracy, and 0.91 for AUC. Similarly for the train-
were much more powerful. The chosen features and the ingdata,precisionwas0.80,recallwas0.78,F1was0.79,
rationale for choosing them will soon be discussed in accuracy was 0.82, and AUC was 0.93. Based on these
greaterdetail,butnotethateventheengineeredversions metrics, there were minor indications of overfitting but
of ”koi quarters” were not used. not a concerning amount.
”koi pdisposition” is the guess from NASA’s auto- Looking at the tree’s feature importances, many fea-
mated system called Robovetter which is rule-based and tures were assigned zero weight. Out of the ones as-5
FIG.5: Thiscodewasusedtotrainthedecisiontreeclassifierthathelpedwithdimensionalityreductionandfeatureselection.
The hyperparameters were varied through GridSearchCV from scikit-learn and are represented in the ”params” dictionary.
K-Fold cross-validation was used with 5 folds, optimizing for precision.
signed non-zero importance, the following were chosen: candidate class was not used for training. This resulted
”koi ror” (0.26 importance), ”koi dikco msky” (0.22 im- in approximately balanced classes and 7301 samples to
portance), and ”koi max mult ev” (0.12 importance). use for training and testing. Roughly, each model was
These were one of the most important, but not neces- trained on 30 time steps with the associated brightness
sarily the top three. The decision was largely aided by for each time, and the three contextual features. Ad-
domainknowledgeandensuringthatthecorrelationma- justmentsweremadedependingonthemodelandthisis
trix did not indicate high collinearity. ”koi ror” is the explored in Section 7.2.
ratio of the planet radius to star radius and was chosen
since it provides information about the size of the tran-
siting object and star. ”koi dikco msky” represents the
7.2. Model-Specific Engineering
difference between the observed position of a star and
its cataloged position, so it gives insight to the uncer-
tainty in positional alignment which helps detect false RNNs can accept tensors as input which allows each
positives that are light interference. ”koi max mult ev” time to be paired with its associated flux so the features
is the maximum signal to noise ratio which helps distin- did not have to be engineered other than reshaping into
guish instrumental noise. These three are considered the acompatibletensor;thespecificshapeswillbeexplained
”contextual” features; they supplement the raw bright- in Section 8 since they are part of the model architec-
ness of a star during a transit and roughly cover all the ture. However, logistic regression and random forest are
types of false positives. limited to 2-dimensional input so instead of having the
times and fluxes as inputs, they were engineered into 29
ThelightcurvesintheSQLdatabaseweretransformed features that represent the slope of the flux. Each fea-
into features. For a given potential planet, 30 time steps ture is the change in flux divided by the change in time
centered around its first detected transit were used as between each pair of adjacent time steps. Time and its
columns in a CSV file; specifically 30 were used due to corresponding flux come in pairs for each time step, so
computationalconstraintsandmostsampleshavingtran- this engineering was done to help the models recognize
sitlengthswithinthiswindow. ThePDCSAPfluxisthe the pairwise relationship since they cannot process fea-
brightness of the star after being processed by NASA’s tures sequentially like RNNs.
pipeline to help remove instrumental effects while keep- The features for the random forest were not scaled
ingthetransits;itwasincludedforeachtimestep. Other since it is an ensemble of decision trees which do not re-
variables were also added to the CSV for each time step quire scaling and could otherwise distort the data. The
but were ultimately unused. Transits that had less than features for the logistic regression were normalized using
30 time steps were not included. As these transits are the standard scaler from scikit-learn since it helps with
periodic, the first four dips were used to augment the convergenceandpreventsfeatureswithlargerscalesfrom
number of samples in the dataset; if an object had less dominating; the standard scaler was preferred over the
than four transits, then the amount of transits it had min-max scaler because it is less sensitive to outliers. In
were used. This process resulted in about 10,318 sam- contrast, the features for the RNN were scaled using the
ples of transits and the table was saved as transits.csv. min-max scaler from scikit-learn; since transit dips can
Originally,theclassificationwasgoingtobeforfalsepos- be small, it is likely best to remain in a consistent range
itives, candidates, and confirmed planets, but due to the for a model that can process the data sequentially. For
data augmentation, there were enough samples to train all models, the target label was ordinally encoded with
a binary problem predicting just false positives and con- 0 representing a false positive and 1 representing a true
firmed planets. This was ideal since ”candidates” are a planet;thisistypicallyrequiredbythelibrariesandmod-
grayareathatworsentheperformanceofthemodels,and elschosen, buttheorderingalsohastheintendedbehav-
with binary decisions, these candidates could be classi- iorofviewingclass1as”more”ofaplanetandclass0as
fiedintoplanetsorfalsepositiveswhichismoreusefulto ”less” of a planet. Categorical features were previously
NASA’s pipeline. Thus, the problem statement became used,buttheywerenotneededorusedfortrainingsince
to classify transits as planets or false positives and the numerical features happened to be the most important6
based on feature selection.
8. DATA MODELING
8.1. Random Forest Classifier
As mentioned, the feature space was customized for
each model, but those resulting sets were all split into
80%fortrainingand20%fortesting. Thissplitwasdone
in an attempt to train on as much data as possible while
still setting samples aside to evaluate generalization to
data that was unseen during training. Stratification was FIG. 6: This code was used to train the random for-
used for the target label even though it was relatively est classifier. The hyperparameters were varied through
GridSearchCV from scikit-learn and are represented in the
balanced to help ensure the training and testing perfor-
”params” dictionary. K-Fold cross-validation was used with
mance could be directly comparable. Other imbalances
5 folds, optimizing for precision.
couldexistbetweenthesetsbutanimbalanceofthetar-
get label would likely be the greatest disturbance. Note
thatallmodelsweresavedaspicklefilesforfutureusein
a ”Models” folder.
Therandomforestclassifierfromscikit-learnwasused
and it classifies by averaging the predictions of an en-
semble of decision trees, each adjusted with some ran-
domness for reduced overfitting. It was hyperparameter
tunedasshowninFigure6. Hyperparameterssuchascri-
terionandmaxtreedepthwerevariedwithK-Foldcross-
validation (5 folds), optimizing for precision. The best
hyperparameters found were the following: gini for ”cri-
terion”, 50 for ”max depth”, 2 for ”min samples leaf”,
and 450 for ”min samples split”. The mean validation
scores across all folds were: 0.93 for precision, 0.91 for
recall, 0.92 for F1, 0.93 for accuracy, and 0.98 for AUC.
The mean training scores across all folds were: 0.94 for
precision, 0.92 for recall, 0.93 for F1, 0.93 for accuracy,
and 0.98 for AUC. There was minimal increased perfor-
mance for the training data compared to the validation FIG. 7: Confusion matrix for random forest classifier on the
data. training data, where 0 represents the false positive (non-
planet) class and 1 is the true/confirmed planet class.
The confusion matrix on the entirety of the training
data is shown in Figure 7. There are fewer misclassifica-
tions for the non-planet class. Figure 8 shows the ROC
8.2. Logistic Regression
curve with an AUC of 0.98, indicating a strong ability
to separate the classes. Figure 9 shows the importance
of each feature for the model to make its decisions. A The logistic regression model from scikit-learn was
largeemphasisisplacedonthethreecontextualfeatures. used and it classifies by feeding a linear combination of
Out of the features that represent the slope of the light the features into a sigmoid function bounded between 0
curve, there are two peaks which correspond to the re- and 1. It was hyperparameter tuned as shown in Figure
gionnearthecenterofatransit. Thiscouldindicatethat 10. Hyperparameterssuchasthepenaltyandsolverwere
themostimportantcharacteristicsareslightlybeforethe varied with K-Fold cross-validation (5 folds), optimizing
center of the transit and slightly after, indicating a need for precision. The best hyperparameters found were the
for depth perception. Another possibility is that non- following: ”C” of 5, ”max iter” of 100, ”penalty” of l1,
planets sometimes have two dips and this could be an andliblinearasthe”solver”. Themeanvalidationscores
indication; samples with two dips might be a clear red across all folds were: 0.88 for precision, 0.96 for recall,
flag. 0.92 for F1, 0.92 for accuracy, and 0.95 for AUC. The7
FIG.8: ReceiverOperatingCharacteristic(ROC)curvewith
FIG. 10: This code was used to train the logistic re-
its associated Area Under the Curve (AUC) for the random
gression model. The hyperparameters were varied through
forest classifier.
GridSearchCV from scikit-learn and are represented in the
”params” dictionary. K-Fold cross-validation was used with
5 folds, optimizing for precision.
FIG. 9: Feature importances of the random forest classifier,
as provided by scikit-learn.
FIG.11: Confusionmatrixforlogisticregressiononthetrain-
ing data, where 0 represents the false positive (non-planet)
mean training scores across all folds were: 0.88 for pre-
class and 1 is the true/confirmed planet class.
cision, 0.97 for recall, 0.92 for F1, 0.92 for accuracy, and
0.96forAUC.Therewasnosignificantdifferencebetween
training and validation performance, likely due to regu-
larization preventing overfitting. 8.3. Recurrent Neural Network
The confusion matrix on the entirety of the training
data is shown in Figure 11. There are fewer misclassi- Figure14showsthestructureoftheRNNmodel,built
fications for the true non-planet class. Figure 12 shows using TensorFlow. The time input was a tensor in the
the ROC curve with an AUC of 0.96, indicating a strong following shape: (number of samples, 30, 2). ”30” rep-
ability to separate the classes. Figure 13 shows the mag- resented the number of time steps and ”2” represented
nitudeofthecoefficientswhichisanalogoustothefeature the time and PDC SAP flux for each time step. The
importances of the random forest. The three contextual contextualinputwasforthethreechosencontextualfea-
featuresareimportant,butthereismuchmorevariability turesandwasatensorofthefollowingshape: (numberof
in the time-based features. This could indicate a greater samples,3,). Thenumberofneuronsanddropoutshown
understanding of the nuances of the light curve, or diffi- were the best hyperparameters found for maximum val-
culty picking out the main patterns. idation precision. 20% of the training data was reserved8
for dropout, LSTM vs. GRU, and (16, 32, 64, 128) for
batch size.
FIG.12: ReceiverOperatingCharacteristic(ROC)curvewith
itsassociatedAreaUndertheCurve(AUC)forlogisticregres-
sion.
FIG.14: StructureoftheRNNmodelwiththebesthyperpa-
rameters shown for each layer, optimizing for precision.
After the best model was found, it was trained on the
entire training dataset for the full 200 epochs without
early stopping. Learning rate and batch size were manu-
FIG. 13: Coefficients of logistic regression, as provided by allychosentobe0.0001and32,respectively,basedonthe
scikit-learn, after retaining their absolute value. shape of the loss function across the epochs. A smooth
curve and a flat-line indicating convergence was sought
after and shown in Figure 15. Validation loss was typ-
for validation and 200 epochs were attempted per con- ically lower than training loss which was an indicator
figuration with early stopping (patience of 5 monitoring that the model was not overfitting. On the last epoch,
the validation loss). Binary cross-entropy was chosen as the training accuracy was 0.91, precision was 0.86, and
the loss function due to its convexity which helps with recall was 0.95. The validation accuracy was 0.92, vali-
convergence. TensorFlow’s Adam optimizer was used. dation precision was 0.90, and validation recall was 0.94.
The output layer was a sigmoid but all other dense lay- Validationperformancewasbetterthanthetrainingper-
erswerearectifiedlinearunit(ReLU).Whentuning,the formance so the model seems to generalize well.
option was given between either using Long Short-Term Figure16showstheconfusionmatrix. Therearefewer
Memory (LSTM) or the Gated Recurrent Unit (GRU). misclassifications for the true planet class. Figure 17
The best model chose LSTM. Batch normalization was showstheROCcurvewithanAUCof0.97whichdemon-
used to prevent distribution shifts and dropout was used strates strong discriminatory power.
to prevent overfitting. 50 configurations were run to find
thebesthyperparameters,randomlychoosingthehyper-
parameters during each iteration. The options for the 8.4. Training Performance Comparison
hyperparameters were the following: (1x10−3, 1x10−4,
1x10−5) for learning rate, (16, 32, 64, 128, 256) for the Randomforesthadfewermisclassificationsforthenon-
number of neurons in a layer, (0, 0.1, 0.2, 0.3, 0.4, 0.5) planet classwhichwas in contrast tothe other two mod-9
came confused. However, we cannot rule out the possi-
bility that logistic regression picked up on more complex
patterns and was possibly not a strong enough model to
decipher or utilize them properly.
Both random forest and logistic regression seemed
to have an over-reliance on the contextual features, al-
though less so for the logistic regression. In terms of
metrics,theRNNandrandomforestwereclosecompeti-
FIG. 15: Training and validation loss of the RNN across the
200 epochs it was trained over. The loss function used was
binary cross-entropy.
FIG.17: ReceiverOperatingCharacteristic(ROC)curvewith
its associated Area Under the Curve (AUC) for the RNN.
tors. Logisticwasnotfarbehindbuttherewascertainlya
gapcomparedtotheothertwo. Ingeneral,randomforest
made the fewest misclassifications and had the greatest
discriminatory power. It also had the highest validation
precision which was the primary metric. Its complex-
ity is also lower than the RNN so at this point, it was
the best model, followed by the RNN and then logistic
regression. It is interesting to point out that NASA’s
Robovetter created the false positive class labels. Recall
FIG. 16: Confusion matrix for RNN on the training data,
where0representsthefalsepositive(non-planet)classand1 that Robovetter is rule-based like a decision tree so ran-
is the true/confirmed planet class. domforestmighthavenaturallybeenabletocapturethis
behaviorthebestsinceitistree-based. Itcouldalsohave
beenbiassincethefeatureimportancesofadecisiontree
els which had fewer misclassifications for the true planet wereusedtohelpwithfeatureselections. RNNstypically
class. Based on AUC, the RNN and random forest had do well with time series data so the fact its performance
the strongest discriminatory power. Random forest also wassoclosetotherandomforestisinteresting. Thereare
paid less attention to the time-based features compared severalpossibilities,includingthisproject’smethodology,
to the logistic regression. This was likely because the thelimitedsizeofthetrainingdata,andthepotentialfor
random forest was able to pick out the most important the false positive class to have inaccuracies since NASA
aspectsofthetimeserieswhilethelogisticregressionbe- had an automated system generate them.
[1] NASA, Kepler by the numbers, cumulative KOI data.
https://science.nasa.gov/resource/ [3] NASA,Bulkdatadownload,https://exoplanetarchive.
nasas-kepler-mission-by-the-numbers/ (2018). ipac.caltech.edu/bulk_data_download/, kepler KOI
[2] NASA, Kepler objects of interest, https:// time series.
exoplanetarchive.ipac.caltech.edu/docs/data.html,# Identification of Exoplanets Using NASA Kepler Light Curve Data

## Project Description
NASA recorded the brightness of stars in an attempt to find planets outside of our solar system through missions titled Kepler and K2. Exoplanets can be identified by looking for dips in brightness which typically means a planet is crossing in front of the star (called a transit). The light curves for thousands of stars were recorded along with contextual information such as whether the transit is a true planet, candidate, or false positive. The motive for this project will be to categorize transits as either true planets or non-planets (false positives). To interact with the final results, a conversational agent will be released.

## General Understanding
For an overview of the project objective, methodology, and findings refer to the *Report* folder which contains information for each milestone. To run the code, ensure files in the *Data* and *Scripts* folders are extracted to your working directory.

## Data Collection
To download the data required, refer to *data_access_info.txt* in the *Data* folder. Then, run *data_collection.ipynb* from *Scripts*. This will fetch the required SQLite database and CSV file. A copy of the required CSV file, *KOI_cumulative.csv*, was provided but the SQLite database will have to be generated through *data_collection.ipynb* due to the large size of the database.

## Data Processing
The report for the first milestone has a macro view of the process. For a more detailed view, there are markdown cells in *data_processing.ipynb* from *Scripts*. To replicate results, reading the markdown cells is highly encouraged. The cleaned version *KOI_cumulative.csv* is saved as *KOI_cumulative_cleaned.csv* in the *Data* folder.

## Feature Selection/Engineering and Model Training
This process is covered in *training.ipynb* from *Scripts*. Three contextual features from *KOI_cumulative.csv* were selected and 30 time steps from a given light curve, centered around the first detected transit, were used (from the SQL database of light curves). For each time step, the associated time, PDC SAP flux (corrected star brightness), etc. were saved as *transits.csv* which can be found in the *Data* folder. A random forest classifier, logistic regression model, and RNN were hyperparameter tuned for precision. This is explained in detail in the Jupyter Notebook and the second milestone report. The trained models are saved as *.pkl* files in the *Models* folder; the RNN also has its history and a dictionary of the chosen hyperparameters saved.

## Testing
The models were evaluated on the test set in *testing.ipynb* from *Scripts*. Potential biases were explored.

## Chatbot
*chatbot_dev.ipynb* from *Scripts* covers the development of the chatbot to answer project-related questions. Unlike the other scripts, the directory structure of the GitHub is used and files generally do not need to be extracted to the working space as long as the working directory is *Scripts*.

## Video Demonstrations
- **Project Overview**: https://youtu.be/yYLAPwx-Lss
- **Chatbot Demo**: TO BE ADDEDDependencies of the Scripts

The following libraries require installation:
- Pandas
- SQLite3
- Numpy
- Matplotlib
- Seaborn
- SciPy
- Scikit-learn
- TensorFlow

To use "data_collection.ipynb", wget needs to be installed on the system. Refer to https://irsa.ipac.caltech.edu/docs/batch_download_help.html

Ensure files in the folders of the repository are extracted to your working directory, otherwise the paths will not be recognized in the scripts!

This file is the content of "testing.ipynb"

Markdown Cell:
## Markdown Content:
# Evaluating Test Set

Markdown Cell:
## Markdown Content:
## Performance and Biases

Markdown Cell:
## Markdown Content:
### Random Forest Classifier

Markdown Cell:
## Markdown Content:
We will begin by evaluating the test set.

Code Cell:
```python
import pandas as pd
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
from sklearn.metrics import roc_auc_score, roc_curve
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import joblib

# Load data
raw_KOI = pd.read_csv('KOI_cumulative.csv', comment='#')
data = pd.read_csv('transits.csv')
data = data.merge(raw_KOI.reset_index()[['kepoi_name', 'koi_disposition', 'koi_ror', 'koi_dikco_msky', 'koi_max_mult_ev']], on='kepoi_name')

# Filter to only get time, flux, and the 3 contextual features
cols = data.columns
for col in cols:
    if (col.find('TIME') != -1 and col.find('CORR') == -1) or (col.find('PDCSAP_FLUX') != -1 and col.find('ERR') == -1) or (col.find('koi_disposition') != -1) or (col.find('koi_ror') != -1) or (col.find('koi_dikco_msky') != -1) or (col.find('koi_max_mult_ev') != -1) or (col.find('kepoi_name') != -1) or (col.find('kepid') != -1):
        pass
    else:
        data = data.drop(columns=col)

# Filter to not include candidate class
data = data[data['koi_disposition'] != 'CANDIDATE'].dropna()

# Engineer Features
n_times = 30
for i in range(n_times-1):
    change_time = (data[f'TIME{i+1}'] - data[f'TIME{i}'])
    change_flux = (data[f'PDCSAP_FLUX{i+1}'] - data[f'PDCSAP_FLUX{i}'])
    flux_over_time = change_flux / change_time
    data[f'FLUX_OVER_TIME{i}'] = flux_over_time

# Drop time and flux since it's no longer needed
for i in range(n_times):
    data = data.drop(columns=[f'TIME{i}', f'PDCSAP_FLUX{i}'])

# Split data into training and testing (80-20)
y = data['koi_disposition'].apply(lambda val: 1 if val == 'CONFIRMED' else 0)
data_train, data_test = train_test_split(data, test_size=0.2, random_state=42, stratify=y)
y_train = data_train['koi_disposition'].apply(lambda val: 1 if val == 'CONFIRMED' else 0)
X_train = data_train.drop(columns=['koi_disposition', 'kepid', 'kepoi_name'])
y_test = data_test['koi_disposition'].apply(lambda val: 1 if val == 'CONFIRMED' else 0)
X_test = data_test.drop(columns=['koi_disposition', 'kepid', 'kepoi_name'])

# Show classification report
model = joblib.load('Models/random_forest.pkl')
y_pred = model.predict(X_test)
print('Classification Report')
print(classification_report(y_test, y_pred))

# Confusion Matrix
y_pred = model.predict(X_test)
confusion = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion)
print('Confusion Matrix')
disp.plot()
plt.show()

# Plot ROC curve
y_probs = model.predict_proba(X_test)[:, 1]
fpr, tpr, thresholds = roc_curve(y_test, y_probs)
auc = roc_auc_score(y_test, y_probs)
plt.plot(fpr, tpr, label=f'AUC: {auc:.2f}')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for Training Data')
plt.grid(linestyle='--')
plt.legend()
plt.show()
```

Markdown Cell:
## Markdown Content:
Now we can explore potential biases by looking at distributions for correct and incorrect predictions.

Code Cell:
```python
# Columns to explore
cols_to_explore = ['koi_period', 'koi_duration', 'koi_ror', 'koi_dikco_msky', 'koi_max_mult_ev']

for class_index in [0, 1]:

    # Getting the data that made correct and incorrect predictions
    is_correct = ((y_test == y_pred) & (y_test == class_index))
    is_wrong = ((y_test != y_pred) & (y_test == class_index))
    correct_names = data_test[is_correct]['kepoi_name'].unique()
    wrong_names = data_test[is_wrong]['kepoi_name'].unique()
    correct_data = raw_KOI[raw_KOI['kepoi_name'].isin(correct_names)]
    wrong_data = raw_KOI[raw_KOI['kepoi_name'].isin(wrong_names)]

    # Plot
    for col in cols_to_explore:

        fig = plt.figure(figsize=(10, 8))

        # Set box plots
        ax1 = fig.add_subplot(2, 2, 1)
        correct_data[col].plot.box()
        ax2 = fig.add_subplot(2, 2, 2, sharey=ax1)
        wrong_data[col].plot.box()
        ax1.set_xlabel('')
        ax1.set_ylabel('')
        ax2.set_xlabel('')
        ax2.set_ylabel('')
        ax1.set_title('Correct')
        ax2.set_title('Incorrect')

        # Set histograms
        ax1 = fig.add_subplot(2, 2, 3)
        correct_data[col].plot.hist(bins=30)
        ax2 = fig.add_subplot(2, 2, 4, sharex=ax1)
        wrong_data[col].plot.hist(bins=30)
        ax1.set_xlabel('')
        ax1.set_ylabel('')
        ax2.set_xlabel('')
        ax2.set_ylabel('')
        ax1.set_title('Correct')
        ax2.set_title('Incorrect')


        # Adjust labels and titles
        fig.supxlabel(f'{col}')
        fig.supylabel('Number of Predictions')
        if class_index == 1:
            fig.suptitle(f'"{col}" Classification Outcomes for True Planets')
        else:
            fig.suptitle(f'"{col}" Classification Outcomes for False Positives')
        plt.tight_layout()
        plt.show()
```

Markdown Cell:
## Markdown Content:
### Logistic Regression

Markdown Cell:
## Markdown Content:
We will make predictions on test set.

Code Cell:
```python
import pandas as pd
from sklearn.metrics import roc_auc_score, roc_curve
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
from sklearn.preprocessing import StandardScaler

# Load data
raw_KOI = pd.read_csv('KOI_cumulative.csv', comment='#')
data = pd.read_csv('transits.csv')
data = data.merge(raw_KOI.reset_index()[['kepoi_name', 'koi_disposition', 'koi_ror', 'koi_dikco_msky', 'koi_max_mult_ev']], on='kepoi_name')

# Filter to only get time, flux, and the 3 contextual features
cols = data.columns
for col in cols:
    if (col.find('TIME') != -1 and col.find('CORR') == -1) or (col.find('PDCSAP_FLUX') != -1 and col.find('ERR') == -1) or (col.find('koi_disposition') != -1) or (col.find('koi_ror') != -1) or (col.find('koi_dikco_msky') != -1) or (col.find('koi_max_mult_ev') != -1) or (col.find('kepoi_name') != -1) or (col.find('kepid') != -1):
        pass
    else:
        data = data.drop(columns=col)

# Filter to not include candidate class
data = data[data['koi_disposition'] != 'CANDIDATE'].dropna()

# Engineer Features
n_times = 30
for i in range(n_times-1):
    change_time = (data[f'TIME{i+1}'] - data[f'TIME{i}'])
    change_flux = (data[f'PDCSAP_FLUX{i+1}'] - data[f'PDCSAP_FLUX{i}'])
    flux_over_time = change_flux / change_time
    data[f'FLUX_OVER_TIME{i}'] = flux_over_time

# Drop time and flux since it's no longer needed
for i in range(n_times):
    data = data.drop(columns=[f'TIME{i}', f'PDCSAP_FLUX{i}'])

# Split data into training and testing (80-20)
y = data['koi_disposition'].apply(lambda val: 1 if val == 'CONFIRMED' else 0)
data_train, data_test = train_test_split(data, test_size=0.2, random_state=42, stratify=y)
y_train = data_train['koi_disposition'].apply(lambda val: 1 if val == 'CONFIRMED' else 0)
X_train = data_train.drop(columns=['koi_disposition', 'kepid', 'kepoi_name'])
y_test = data_test['koi_disposition'].apply(lambda val: 1 if val == 'CONFIRMED' else 0)
X_test = data_test.drop(columns=['koi_disposition', 'kepid', 'kepoi_name'])

# Scale
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_train = pd.DataFrame(X_train, columns=scaler.get_feature_names_out())
X_test = scaler.transform(X_test)
X_test = pd.DataFrame(X_test, columns=scaler.get_feature_names_out())


# Show classification report
model = joblib.load('Models/logistic_regression.pkl')
y_pred = model.predict(X_test)
print('Classification Report')
print(classification_report(y_test, y_pred))

# Confusion Matrix
y_pred = model.predict(X_test)
confusion = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion)
print('Confusion Matrix')
disp.plot()
plt.show()

# Plot ROC curve
y_probs = model.predict_proba(X_test)[:, 1]
fpr, tpr, thresholds = roc_curve(y_test, y_probs)
auc = roc_auc_score(y_test, y_probs)
plt.plot(fpr, tpr, label=f'AUC: {auc:.2f}')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for Training Data')
plt.grid(linestyle='--')
plt.legend()
plt.show()
```

Markdown Cell:
## Markdown Content:
We will show distributions of the predictions.

Code Cell:
```python
# Columns to explore
cols_to_explore = ['koi_period', 'koi_duration', 'koi_ror', 'koi_dikco_msky', 'koi_max_mult_ev']

for class_index in [0, 1]:

    # Getting the data that made correct and incorrect predictions
    is_correct = ((y_test == y_pred) & (y_test == class_index))
    is_wrong = ((y_test != y_pred) & (y_test == class_index))
    correct_names = data_test[is_correct]['kepoi_name'].unique()
    wrong_names = data_test[is_wrong]['kepoi_name'].unique()
    correct_data = raw_KOI[raw_KOI['kepoi_name'].isin(correct_names)]
    wrong_data = raw_KOI[raw_KOI['kepoi_name'].isin(wrong_names)]

    # Plot
    for col in cols_to_explore:

        fig = plt.figure(figsize=(10, 8))

        # Set box plots
        ax1 = fig.add_subplot(2, 2, 1)
        correct_data[col].plot.box()
        ax2 = fig.add_subplot(2, 2, 2, sharey=ax1)
        wrong_data[col].plot.box()
        ax1.set_xlabel('')
        ax1.set_ylabel('')
        ax2.set_xlabel('')
        ax2.set_ylabel('')
        ax1.set_title('Correct')
        ax2.set_title('Incorrect')

        # Set histograms
        ax1 = fig.add_subplot(2, 2, 3)
        correct_data[col].plot.hist(bins=30)
        ax2 = fig.add_subplot(2, 2, 4, sharex=ax1)
        wrong_data[col].plot.hist(bins=30)
        ax1.set_xlabel('')
        ax1.set_ylabel('')
        ax2.set_xlabel('')
        ax2.set_ylabel('')
        ax1.set_title('Correct')
        ax2.set_title('Incorrect')


        # Adjust labels and titles
        fig.supxlabel(f'{col}')
        fig.supylabel('Number of Predictions')
        if class_index == 1:
            fig.suptitle(f'"{col}" Classification Outcomes for True Planets')
        else:
            fig.suptitle(f'"{col}" Classification Outcomes for False Positives')
        plt.tight_layout()
        plt.show()
```

Markdown Cell:
## Markdown Content:
## RNN

Markdown Cell:
## Markdown Content:
We will make predictions on test set.

Code Cell:
```python
import pandas as pd
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
from sklearn.metrics import roc_auc_score, roc_curve
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
import numpy as np

# Load data
raw_KOI = pd.read_csv('KOI_cumulative.csv', comment='#')
data = pd.read_csv('transits.csv')
data = data.merge(raw_KOI.reset_index()[['kepoi_name', 'koi_disposition', 'koi_ror', 'koi_dikco_msky', 'koi_max_mult_ev']], on='kepoi_name')

# Filter to only get time, flux, and the 3 contextual features
cols = data.columns
for col in cols:
    if (col.find('TIME') != -1 and col.find('CORR') == -1) or (col.find('PDCSAP_FLUX') != -1 and col.find('ERR') == -1) or (col.find('koi_disposition') != -1) or (col.find('koi_ror') != -1) or (col.find('koi_dikco_msky') != -1) or (col.find('koi_max_mult_ev') != -1) or (col.find('kepoi_name') != -1) or (col.find('kepid') != -1):
        pass
    else:
        data = data.drop(columns=col)

# Filter to not include candidate class
data = data[data['koi_disposition'] != 'CANDIDATE'].dropna()

# Split data into training and testing (80-20)
y = data['koi_disposition'].apply(lambda val: 1 if val == 'CONFIRMED' else 0)
data_train, data_test = train_test_split(data, test_size=0.2, random_state=42, stratify=y)
y_train = data_train['koi_disposition'].apply(lambda val: 1 if val == 'CONFIRMED' else 0)
X_train = data_train.drop(columns=['koi_disposition', 'kepid', 'kepoi_name'])
y_test = data_test['koi_disposition'].apply(lambda val: 1 if val == 'CONFIRMED' else 0)
X_test = data_test.drop(columns=['koi_disposition', 'kepid', 'kepoi_name'])

# Scale
scaler = MinMaxScaler()
X_train = scaler.fit_transform(X_train)
X_train = pd.DataFrame(X_train, columns=scaler.get_feature_names_out())
X_test = scaler.transform(X_test)
X_test = pd.DataFrame(X_test, columns=scaler.get_feature_names_out())

# Format X_train and y_train into numpy arrays
time_cols = []
flux_cols = []

for col in X_test.columns:
    if (col.find('TIME') != -1):
        time_cols.append(col)
    elif (col.find('PDCSAP_FLUX') != -1):
        flux_cols.append(col)

X_test_time = X_test[time_cols].to_numpy()
X_test_flux = X_test[flux_cols].to_numpy()
X_test_timesteps = np.stack((X_test_time, X_test_flux), axis=-1)
X_test_context = X_test[['koi_ror', 'koi_dikco_msky', 'koi_max_mult_ev']].to_numpy()
y_test = y_test.to_numpy()

# Show classification report
model = joblib.load('Models/RNN.pkl')
y_pred = model.predict([X_test_timesteps, X_test_context])
y_pred = (y_pred > 0.5).astype(int)
print('Classification Report')
print(classification_report(y_test, y_pred))

# Confusion Matrix
confusion = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion)
print('Confusion Matrix')
disp.plot()
plt.show()

# Plot ROC curve
y_probs = model.predict([X_test_timesteps, X_test_context])
fpr, tpr, thresholds = roc_curve(y_test, y_probs)
auc = roc_auc_score(y_test, y_probs)
plt.plot(fpr, tpr, label=f'AUC: {auc:.2f}')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for Training Data')
plt.grid(linestyle='--')
plt.legend()
plt.show()
```

Markdown Cell:
## Markdown Content:
Now we will explore distributions.

Code Cell:
```python
# Columns to explore
cols_to_explore = ['koi_period', 'koi_duration', 'koi_ror', 'koi_dikco_msky', 'koi_max_mult_ev']

for class_index in [0, 1]:

    # Getting the data that made correct and incorrect predictions
    is_correct = ((y_test == y_pred) & (y_test == class_index))
    is_wrong = ((y_test != y_pred) & (y_test == class_index))
    correct_names = data_test[is_correct]['kepoi_name'].unique()
    wrong_names = data_test[is_wrong]['kepoi_name'].unique()
    correct_data = raw_KOI[raw_KOI['kepoi_name'].isin(correct_names)]
    wrong_data = raw_KOI[raw_KOI['kepoi_name'].isin(wrong_names)]

    # Plot
    for col in cols_to_explore:

        fig = plt.figure(figsize=(10, 8))

        # Set box plots
        ax1 = fig.add_subplot(2, 2, 1)
        correct_data[col].plot.box()
        ax2 = fig.add_subplot(2, 2, 2, sharey=ax1)
        wrong_data[col].plot.box()
        ax1.set_xlabel('')
        ax1.set_ylabel('')
        ax2.set_xlabel('')
        ax2.set_ylabel('')
        ax1.set_title('Correct')
        ax2.set_title('Incorrect')

        # Set histograms
        ax1 = fig.add_subplot(2, 2, 3)
        correct_data[col].plot.hist(bins=30)
        ax2 = fig.add_subplot(2, 2, 4, sharex=ax1)
        wrong_data[col].plot.hist(bins=30)
        ax1.set_xlabel('')
        ax1.set_ylabel('')
        ax2.set_xlabel('')
        ax2.set_ylabel('')
        ax1.set_title('Correct')
        ax2.set_title('Incorrect')


        # Adjust labels and titles
        fig.supxlabel(f'{col}')
        fig.supylabel('Number of Predictions')
        if class_index == 1:
            fig.suptitle(f'"{col}" Classification Outcomes for True Planets')
        else:
            fig.suptitle(f'"{col}" Classification Outcomes for False Positives')
        plt.tight_layout()
        plt.show()
```



This file is the content of "training.ipynb"

Markdown Cell:
## Markdown Content:
# Feature Engineering/Selection and Training Models

Markdown Cell:
## Markdown Content:
## Feature Engineering/Selection: KOI Data

Markdown Cell:
## Markdown Content:
### Import KOI Data

Code Cell:
```python
import pandas as pd
import numpy as np

KOI_df = pd.read_csv('KOI_cumulative_cleaned.csv')
```

Markdown Cell:
## Markdown Content:
### Preparing Categorical Features and Preliminary Feature Reduction

Code Cell:
```python
# Convert object types to category
object_cols = KOI_df.select_dtypes(exclude='number').columns.to_list()
KOI_df[object_cols] = KOI_df[object_cols].astype('category')
```

Code Cell:
```python
# Show uniqueness of categorical features
KOI_df.select_dtypes(exclude='number').nunique()
```

Markdown Cell:
## Markdown Content:
"kepoi_name" is entirely unique and will not be informative so it will be kept for identification purposes but dropped as a feature when training. "kepid" has a slightly lower cardinality but is also identifier and the same treatment will follow. "koi_quarters" is in a binary string format that could be better represented as 32 features, each representing a quarter. Another way to transform "koi_quarters" would be to make a new feature that represents the number of quarters that transit was measured across.

Code Cell:
```python
# Turn koi_quarters into 32 features, each representing a quarter
num_quarters = 0
for binary_str in KOI_df['koi_quarters'].values:
    if len(binary_str) > num_quarters:
        num_quarters = len(binary_str)

for i in range(num_quarters):
    KOI_df[f'koi_quarters_{i+1}'] = [0 for _ in range(KOI_df.shape[0])]

for binary_str in KOI_df['koi_quarters'].values:
    for i in range(len(binary_str)):
        KOI_df[f'koi_quarters_{i+1}'] = int(binary_str[i])
```

Code Cell:
```python
# Make a new feature from koi_quarters that represents the number of quarters
KOI_df['num_quarters'] = [0 for _ in range(KOI_df.shape[0])]

for i, binary_str in KOI_df['koi_quarters'].items():
    num_quarters_searched = sum([int(digit) for digit in binary_str])
    KOI_df.loc[i, 'num_quarters'] = num_quarters_searched
```

Code Cell:
```python
# Drop koi_quarters
KOI_df = KOI_df.drop(columns='koi_quarters')
```

Markdown Cell:
## Markdown Content:
"koi_pdisposition" should be dropped since it is the guess from NASA's own automated system (based on rules but not a machine learning model). It will likely be a very strong predictor of "koi_disposition" but is not ideal for this project since part of the interest is to see how the models can compare to NASA's predictions. The same can be said for "koi_score" and false positive flags since they come from Robovetter (NASA's rule-based classifier). Their inclusion will be reconsidered depending on model performance.

Code Cell:
```python
# Drop koi_pdisposition and koi_score
KOI_df = KOI_df.drop(columns=['koi_pdisposition', 'koi_score'])

# Drop false positive flags
cols = KOI_df.columns
for col in cols:
    if col.find('fpflag') != -1:
        print(f'Flag will be dropped: {col}')
        KOI_df = KOI_df.drop(columns=col)
```

Markdown Cell:
## Markdown Content:
Features related to errors will be dropped because they typically have high collinearity, include noise, and are not very interpretable. They could give insights into the distribution of the variable they correspond to, but there is a chance these error features will prevent patterns in more interpretable (and potentially more important) features from shining through.

Code Cell:
```python
# Drop error-related features
cols = KOI_df.columns
for col in cols:
    if col.find('err') != -1:
        print(f'Error will be dropped: {col}')
        KOI_df = KOI_df.drop(columns=col)
```

Markdown Cell:
## Markdown Content:
At this point, the categorical features will be encoded so that techniques for dimensionality reduction can be employed.

Code Cell:
```python
from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder
from sklearn.compose import ColumnTransformer

# Encode categorical features
cat_cols = KOI_df.select_dtypes(exclude='number').drop(columns=['koi_disposition', 'kepoi_name']).columns.to_list()
encoder = ColumnTransformer([('one_hot', OneHotEncoder(), cat_cols), ('ordinal', OrdinalEncoder(categories=[['FALSE POSITIVE', 'CANDIDATE', 'CONFIRMED']]), ['koi_disposition'])], remainder='passthrough')
encoded_KOI = encoder.fit_transform(KOI_df)

col_names = []
for col in encoder.get_feature_names_out():
    core_name = col.split('__')[-1]
    col_names.append(core_name)

encoded_KOI = pd.DataFrame(encoded_KOI, columns=col_names)
```

Code Cell:
```python
print('Encoded KOI')
display(encoded_KOI)
```

Markdown Cell:
## Markdown Content:
### Using Tree-Based Feature Importance to Reduce Dimensionality

Code Cell:
```python
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import make_scorer, precision_score, accuracy_score, recall_score, f1_score, roc_auc_score

# Split data
X = encoded_KOI.drop(columns=['kepid', 'kepoi_name', 'koi_disposition'])
y = encoded_KOI['koi_disposition'].astype(int)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Hyperparameter tune
scoring = {'roc_auc': make_scorer(roc_auc_score, needs_proba=True, multi_class='ovo', average='macro'),
           'precision': make_scorer(precision_score, average='macro'),
           'accuracy': make_scorer(accuracy_score),
           'recall': make_scorer(recall_score, average='macro'),
           'f1': make_scorer(f1_score, average='macro')}
params = {'criterion': ['gini', 'entropy', 'log_loss'], 'max_depth': [None, 1, 2, 5, 10, 50, 200], 'min_samples_split': [2, 10, 200, 500, 1000], 'min_samples_leaf': [1, 2, 3, 5, 20, 100]}
grid = GridSearchCV(estimator=DecisionTreeClassifier(random_state=42), param_grid=params, scoring=scoring, refit='precision', cv=5, n_jobs=-1, verbose=1, return_train_score=True)
grid.fit(X_train, y_train)

# Print best parameters and score
print('Best Parameters:')
print(grid.best_params_)
print()

print('Best Mean Scores: Validation')
print('----------------------------')
print(f"Precision: {grid.cv_results_['mean_test_precision'][grid.best_index_]}")
print(f"Recall: {grid.cv_results_['mean_test_recall'][grid.best_index_]}")
print(f"F1: {grid.cv_results_['mean_test_f1'][grid.best_index_]}")
print(f"Accuracy: {grid.cv_results_['mean_test_accuracy'][grid.best_index_]}")
print(f"AUC: {grid.cv_results_['mean_test_roc_auc'][grid.best_index_]}")
print()

print('Best Mean Scores: Training')
print('----------------------------')
print(f"Precision: {grid.cv_results_['mean_train_precision'][grid.best_index_]}")
print(f"Recall: {grid.cv_results_['mean_train_recall'][grid.best_index_]}")
print(f"F1: {grid.cv_results_['mean_train_f1'][grid.best_index_]}")
print(f"Accuracy: {grid.cv_results_['mean_train_accuracy'][grid.best_index_]}")
print(f"AUC: {grid.cv_results_['mean_train_roc_auc'][grid.best_index_]}")
```

Code Cell:
```python
# Putting feature importances in descending order
best_model = grid.best_estimator_
feature_importances = list(enumerate(np.abs(best_model.feature_importances_)))
feature_importances = sorted(feature_importances, key=lambda x: x[1], reverse=True)

features = X_train.columns.to_list()
for i in range(len(feature_importances)):
    (index, value) = feature_importances[i]
    feature_importances[i] = (features[index], value)

print('Feature Importances')
print('----------------------')
print(feature_importances)
```

Markdown Cell:
## Markdown Content:
The following features will be chosen due to their ability to help distinguish the types of false positives and provide context:

- koi_ror: helps with size of transiting object and star
- koi_dikco_msky: helps with uncertainty of positional alignment
- koi_max_mult_ev: helps with signal to noise ratio

Code Cell:
```python
# Select most important features
encoded_KOI = encoded_KOI[['kepid', 'kepoi_name', 'koi_ror', 'koi_dikco_msky', 'koi_max_mult_ev']]
display(encoded_KOI)
```

Markdown Cell:
## Markdown Content:
## Feature Engineering/Selection: Light Curve Data

Markdown Cell:
## Markdown Content:
It is important to highlight that this is a general pipeline for feature engineering and reduction, but the exact features for each model will vary. Although similar, each model has its own features that may be scaled differently to better suit the model.

Markdown Cell:
## Markdown Content:
### Select Portions of Light Curves

Code Cell:
```python
import pandas as pd

# Import raw data because it has original scales
raw_KOI = pd.read_csv('KOI_cumulative.csv', comment='#')
raw_KOI = raw_KOI[raw_KOI['kepid'].isin(KOI_df['kepid'].values)]
raw_KOI = raw_KOI.set_index(keys='kepoi_name')
```

Markdown Cell:
## Markdown Content:
The goal is to put 30 time measurements and their associated flux for each transit into a data frame.

Code Cell:
```python
import sqlite3
import csv
import os

def make_transit_csv(file_name, num_measurements=30, max_num_repeated_transits=1):

    """
    Parameters:
        file_name: desired name of csv file that will be output
        num_measurements=30: how many time steps to include from the light curve
        max_num_repeated_transits: for a given event, determines many of its repetitions will be selected
    """

    # Connect to database
    conn = sqlite3.connect('light_curves.db')
    cursor = conn.cursor()
    cursor.execute('PRAGMA cache_size = 1000000')

    # Make header of csv

    with open(file_name, mode='w', newline='') as file:
        writer = csv.writer(file)
        header = ['kepoi_name', 'kepid']

        for col in ['TIME', 'TIMECORR', 'PDCSAP_FLUX', 'PDCSAP_FLUX_ERR', 'SAP_QUALITY']:

            header += [f'{col}{i}' for i in range(num_measurements)]

        writer.writerow(header)

    # Time between measurements is 30 minutes (convert to days)
    cadence = 30 / 60 / 24

    # For each transit, insert light curve into data frame
    for name in encoded_KOI['kepoi_name'].values:

        # Fetch light curve
        row = raw_KOI.loc[name]
        transit = row['koi_time0bk']
        kepid = row['kepid']
        period = row['koi_period']
        allowed_error = 0.05

        # Adds first few transits (limited by max_num_repeated_transits)
        for i in range(max_num_repeated_transits):

            max_time = transit + (cadence * num_measurements/2) * (1 + allowed_error)
            min_time = transit - (cadence * num_measurements/2) * (1 + allowed_error)

            query = f"""
                SELECT *
                FROM LightCurve
                WHERE (TIME IS NOT NULL) AND (PDCSAP_FLUX IS NOT NULL) AND (KEP_ID = {kepid}) AND (TIME BETWEEN {min_time} AND {max_time})
                ORDER BY TIME ASC;
                """

            result = cursor.execute(query).fetchall()

            # If there's too many data points then truncate (from both sides)
            while (len(result) > num_measurements):

                del result[0]

                if len(result) > num_measurements:
                    del result[-1]

            # If there's not enough data points then don't include
            # Format result to be insertable into csv
            transit_data = ()

            if len(result) == num_measurements:

                id, time, time_corr, flux, flux_err, quality = zip(*result)
                transit_data = (name, kepid) + time + time_corr + flux + flux_err + quality

                with open(file_name, mode='a', newline='') as file:
                    writer = csv.writer(file)
                    writer.writerow(transit_data)

            transit += period

# Make csv where each row is a transit with its flux, flux error, etc. in a selected time windowa
file_name = 'transits.csv'

if not os.path.exists(file_name):

    make_transit_csv(file_name, num_measurements=30, max_num_repeated_transits=4)
```

Markdown Cell:
## Markdown Content:
**IMPORTANT**: There are enough samples for planets and false positives so the classification will now be binary. The prediction will be whether the transit is a planet or false positive ... the candidate class will be dropped from now on.

Markdown Cell:
## Markdown Content:
## Model Training: Random Forest Classifier

Code Cell:
```python
import pandas as pd

# Load data
raw_KOI = pd.read_csv('KOI_cumulative.csv', comment='#')
data = pd.read_csv('transits.csv')
data = data.merge(raw_KOI.reset_index()[['kepoi_name', 'koi_disposition', 'koi_ror', 'koi_dikco_msky', 'koi_max_mult_ev']], on='kepoi_name')
print('Data')
display(data.head())
```

Code Cell:
```python
# Filter to only get time, flux, and the 3 contextual features
cols = data.columns
for col in cols:
    if (col.find('TIME') != -1 and col.find('CORR') == -1) or (col.find('PDCSAP_FLUX') != -1 and col.find('ERR') == -1) or (col.find('koi_disposition') != -1) or (col.find('koi_ror') != -1) or (col.find('koi_dikco_msky') != -1) or (col.find('koi_max_mult_ev') != -1) or (col.find('kepoi_name') != -1) or (col.find('kepid') != -1):
        pass
    else:
        data = data.drop(columns=col)

# Filter to not include candidate class
data = data[data['koi_disposition'] != 'CANDIDATE'].dropna()
print('Data filtered')
display(data.head())
```

Markdown Cell:
## Markdown Content:
Time and flux measurements will be converted into features that are change in flux over change in time to give the slope of the light curve.

Code Cell:
```python
# Engineer Features
n_times = 30
for i in range(n_times-1):
    change_time = (data[f'TIME{i+1}'] - data[f'TIME{i}'])
    change_flux = (data[f'PDCSAP_FLUX{i+1}'] - data[f'PDCSAP_FLUX{i}'])
    flux_over_time = change_flux / change_time
    data[f'FLUX_OVER_TIME{i}'] = flux_over_time

# Drop time and flux since it's no longer needed
for i in range(n_times):
    data = data.drop(columns=[f'TIME{i}', f'PDCSAP_FLUX{i}'])
```

Code Cell:
```python
print('Engineered Data')
display(data.head())
```

Markdown Cell:
## Markdown Content:
No scaling will be used since random forest uses trees which do not require it.

Code Cell:
```python
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import make_scorer, precision_score, accuracy_score, recall_score, f1_score, roc_auc_score
from sklearn.ensemble import RandomForestClassifier
import joblib

# Split data into training and testing (80-20)
y = data['koi_disposition'].apply(lambda val: 1 if val == 'CONFIRMED' else 0)
data_train, data_test = train_test_split(data, test_size=0.2, random_state=42, stratify=y)
y_train = data_train['koi_disposition'].apply(lambda val: 1 if val == 'CONFIRMED' else 0)
X_train = data_train.drop(columns=['koi_disposition', 'kepid', 'kepoi_name'])
y_test = data_test['koi_disposition'].apply(lambda val: 1 if val == 'CONFIRMED' else 0)
X_test = data_test.drop(columns=['koi_disposition', 'kepid', 'kepoi_name'])
print('X_train')
display(X_train.head())
print('y_train')
display(y_train.head())

# Tune
scoring = {'roc_auc': 'roc_auc',
           'precision': 'precision',
           'accuracy': 'accuracy',
           'recall': 'recall',
           'f1': 'f1'}

params = {'criterion': ['gini', 'entropy'], 
          'max_depth': [2, 5, 10, 50, 100, 200], 
          'min_samples_split': [350, 400, 450, 500, 550], 
          'min_samples_leaf': [2, 3, 5, 20]}

grid = GridSearchCV(estimator=RandomForestClassifier(random_state=42), 
                    param_grid=params,
                      scoring=scoring, 
                      refit='precision', 
                      cv=5, 
                      n_jobs=-1, 
                      verbose=1, 
                      return_train_score=True)

grid.fit(X_train, y_train)

# Print best parameters and score
print('Best Parameters:')
print(grid.best_params_)
print()

print('Best Mean Scores Across Splits: Validation')
print('----------------------------')
print(f"Precision: {grid.cv_results_['mean_test_precision'][grid.best_index_]}")
print(f"Recall: {grid.cv_results_['mean_test_recall'][grid.best_index_]}")
print(f"F1: {grid.cv_results_['mean_test_f1'][grid.best_index_]}")
print(f"Accuracy: {grid.cv_results_['mean_test_accuracy'][grid.best_index_]}")
print(f"AUC: {grid.cv_results_['mean_test_roc_auc'][grid.best_index_]}")
print()

print('Best Mean Scores Across Splits: Training')
print('----------------------------')
print(f"Precision: {grid.cv_results_['mean_train_precision'][grid.best_index_]}")
print(f"Recall: {grid.cv_results_['mean_train_recall'][grid.best_index_]}")
print(f"F1: {grid.cv_results_['mean_train_f1'][grid.best_index_]}")
print(f"Accuracy: {grid.cv_results_['mean_train_accuracy'][grid.best_index_]}")
print(f"AUC: {grid.cv_results_['mean_train_roc_auc'][grid.best_index_]}")

# Save model
joblib.dump(grid.best_estimator_, 'random_forest.pkl')
```

Code Cell:
```python
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay

# Show classification report
best_model = grid.best_estimator_
y_pred = best_model.predict(X_train)
print('Classification Report')
print(classification_report(y_train, y_pred))

# Confusion Matrix
y_pred = best_model.predict(X_train)
confusion = confusion_matrix(y_train, y_pred)
disp = ConfusionMatrixDisplay(confusion)
print('Confusion Matrix')
disp.plot()
```

Code Cell:
```python
from sklearn.metrics import roc_auc_score, roc_curve
import matplotlib.pyplot as plt

# Plot ROC curve
y_probs = best_model.predict_proba(X_train)[:, 1]
fpr, tpr, thresholds = roc_curve(y_train, y_probs)
auc = roc_auc_score(y_train, y_probs)
plt.plot(fpr, tpr, label=f'AUC: {auc:.2f}')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for Training Data')
plt.grid(linestyle='--')
plt.legend()
plt.show()
```

Code Cell:
```python
# Show feature importances
plt.figure(figsize=(10,6))
plt.plot(range(len(grid.best_estimator_.feature_importances_)), grid.best_estimator_.feature_importances_, marker='o')
plt.xlabel('Feature', labelpad=15)
plt.ylabel('Weight (Importance)')
plt.xticks(ticks=range(len(X_train.columns)), labels=[X_train.columns[i] for i in range(len(X_train.columns))], rotation=90)
plt.grid(linestyle='--')
plt.title('Feature Importances for Random Forest')
plt.show()
```

Markdown Cell:
## Markdown Content:
## Model Training: Logistic Regression Classifier

Code Cell:
```python
import pandas as pd

# Load data
raw_KOI = pd.read_csv('KOI_cumulative.csv', comment='#')
data = pd.read_csv('transits.csv')
data = data.merge(raw_KOI.reset_index()[['kepoi_name', 'koi_disposition', 'koi_ror', 'koi_dikco_msky', 'koi_max_mult_ev']], on='kepoi_name')
print('Data')
display(data.head())
```

Code Cell:
```python
# Filter to only get time, flux, and the 3 contextual features
cols = data.columns
for col in cols:
    if (col.find('TIME') != -1 and col.find('CORR') == -1) or (col.find('PDCSAP_FLUX') != -1 and col.find('ERR') == -1) or (col.find('koi_disposition') != -1) or (col.find('koi_ror') != -1) or (col.find('koi_dikco_msky') != -1) or (col.find('koi_max_mult_ev') != -1) or (col.find('kepoi_name') != -1) or (col.find('kepid') != -1):
        pass
    else:
        data = data.drop(columns=col)

# Filter to not include candidate class
data = data[data['koi_disposition'] != 'CANDIDATE'].dropna()
print('Data filtered')
display(data.head())
```

Markdown Cell:
## Markdown Content:
Time and flux measurements will be converted into features that are change in flux over change in time to give the slope of the light curve.

Code Cell:
```python
# Engineer Features
n_times = 30
for i in range(n_times-1):
    change_time = (data[f'TIME{i+1}'] - data[f'TIME{i}'])
    change_flux = (data[f'PDCSAP_FLUX{i+1}'] - data[f'PDCSAP_FLUX{i}'])
    flux_over_time = change_flux / change_time
    data[f'FLUX_OVER_TIME{i}'] = flux_over_time

# Drop time and flux since it's no longer needed
for i in range(n_times):
    data = data.drop(columns=[f'TIME{i}', f'PDCSAP_FLUX{i}'])
```

Code Cell:
```python
print('Engineered Data')
display(data.head())
```

Markdown Cell:
## Markdown Content:
The features will be scaled using standard scaler since it will help with convergence.

Code Cell:
```python
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import make_scorer, precision_score, accuracy_score, recall_score, f1_score, roc_auc_score
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

# Split data into training and testing (80-20)
y = data['koi_disposition'].apply(lambda val: 1 if val == 'CONFIRMED' else 0)
data_train, data_test = train_test_split(data, test_size=0.2, random_state=42, stratify=y)
y_train = data_train['koi_disposition'].apply(lambda val: 1 if val == 'CONFIRMED' else 0)
X_train = data_train.drop(columns=['koi_disposition', 'kepid', 'kepoi_name'])
y_test = data_test['koi_disposition'].apply(lambda val: 1 if val == 'CONFIRMED' else 0)
X_test = data_test.drop(columns=['koi_disposition', 'kepid', 'kepoi_name'])
print('X_train')
display(X_train.head())
print('y_train')
display(y_train.head())

# Scale
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_train = pd.DataFrame(X_train, columns=scaler.get_feature_names_out())
X_test = scaler.transform(X_test)
X_test = pd.DataFrame(X_test, columns=scaler.get_feature_names_out())

# Tune
scoring = {'roc_auc': 'roc_auc',
           'precision': 'precision',
           'accuracy': 'accuracy',
           'recall': 'recall',
           'f1': 'f1'}
params = {'C': [0.1, 1, 5, 10, 15, 20], 'penalty': ['l1', 'l2'], 'max_iter': [100, 200, 500], 'solver': ['lbfgs', 'liblinear', 'saga']}
grid = GridSearchCV(estimator=LogisticRegression(random_state=42), param_grid=params, scoring=scoring, refit='precision', cv=5, n_jobs=-1, verbose=1, return_train_score=True)
grid.fit(X_train, y_train)

# Print best parameters and score
print('Best Parameters:')
print(grid.best_params_)
print()

print('Best Mean Scores Across Splits: Validation')
print('----------------------------')
print(f"Precision: {grid.cv_results_['mean_test_precision'][grid.best_index_]}")
print(f"Recall: {grid.cv_results_['mean_test_recall'][grid.best_index_]}")
print(f"F1: {grid.cv_results_['mean_test_f1'][grid.best_index_]}")
print(f"Accuracy: {grid.cv_results_['mean_test_accuracy'][grid.best_index_]}")
print(f"AUC: {grid.cv_results_['mean_test_roc_auc'][grid.best_index_]}")
print()

print('Best Mean Scores Across Splits: Training')
print('----------------------------')
print(f"Precision: {grid.cv_results_['mean_train_precision'][grid.best_index_]}")
print(f"Recall: {grid.cv_results_['mean_train_recall'][grid.best_index_]}")
print(f"F1: {grid.cv_results_['mean_train_f1'][grid.best_index_]}")
print(f"Accuracy: {grid.cv_results_['mean_train_accuracy'][grid.best_index_]}")
print(f"AUC: {grid.cv_results_['mean_train_roc_auc'][grid.best_index_]}")

# Save model
joblib.dump(grid.best_estimator_, 'logistic_regression.pkl')
```

Code Cell:
```python
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay

# Show classification report
best_model = grid.best_estimator_
y_pred = best_model.predict(X_train)
print('Classification Report')
print(classification_report(y_train, y_pred))

# Confusion Matrix
y_pred = best_model.predict(X_train)
confusion = confusion_matrix(y_train, y_pred)
disp = ConfusionMatrixDisplay(confusion)
print('Confusion Matrix')
disp.plot()
```

Code Cell:
```python
from sklearn.metrics import roc_auc_score, roc_curve
import matplotlib.pyplot as plt

# Plot ROC curve
y_probs = best_model.predict_proba(X_train)[:, 1]
fpr, tpr, thresholds = roc_curve(y_train, y_probs)
auc = roc_auc_score(y_train, y_probs)
plt.plot(fpr, tpr, label=f'AUC: {auc:.2f}')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for Training Data')
plt.grid(linestyle='--')
plt.legend()
plt.show()
```

Code Cell:
```python
# Show feature importances
plt.figure(figsize=(10,6))
plt.plot(range(len(grid.best_estimator_.coef_.flatten())), np.abs(grid.best_estimator_.coef_.flatten()), marker='o')
plt.xlabel('Feature', labelpad=15)
plt.ylabel('Magnitude of Coefficients for Logistic Regression')
plt.xticks(ticks=range(len(X_train.columns)), labels=[X_train.columns[i] for i in range(len(X_train.columns))], rotation=90)
plt.grid(linestyle='--')
plt.title('Magnitude of Coefficients')
plt.show()
```

Markdown Cell:
## Markdown Content:
## Model Training: RNN

Code Cell:
```python
import pandas as pd

# Load data
raw_KOI = pd.read_csv('KOI_cumulative.csv', comment='#')
data = pd.read_csv('transits.csv')
data = data.merge(raw_KOI.reset_index()[['kepoi_name', 'koi_disposition', 'koi_ror', 'koi_dikco_msky', 'koi_max_mult_ev']], on='kepoi_name')
print('Data')
display(data.head())
```

Code Cell:
```python
# Filter to only get time, flux, and the 3 contextual features
cols = data.columns
for col in cols:
    if (col.find('TIME') != -1 and col.find('CORR') == -1) or (col.find('PDCSAP_FLUX') != -1 and col.find('ERR') == -1) or (col.find('koi_disposition') != -1) or (col.find('koi_ror') != -1) or (col.find('koi_dikco_msky') != -1) or (col.find('koi_max_mult_ev') != -1) or (col.find('kepoi_name') != -1) or (col.find('kepid') != -1):
        pass
    else:
        data = data.drop(columns=col)

# Filter to not include candidate class
data = data[data['koi_disposition'] != 'CANDIDATE'].dropna()
print('Data filtered')
display(data.head())
```

Markdown Cell:
## Markdown Content:
The features will be scaled using min-max since it will help with convergence and neural networks often do best with [0,1] range. The model will be hypertuned for precision by using random choices for the number of neurons in a layer, learning rate, etc. This hypertuning will be done with early stopping and 200 epochs; the best model will later be trained across the full 200 epochs without early stopping to further reduce validation loss.

Code Cell:
```python
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, LSTM, Dense, Concatenate, Dropout, BatchNormalization, GRU
from sklearn.model_selection import train_test_split
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
import numpy as np
import joblib
import random

# Split data into training and testing (80-20)
y = data['koi_disposition'].apply(lambda val: 1 if val == 'CONFIRMED' else 0)
data_train, data_test = train_test_split(data, test_size=0.2, random_state=42, stratify=y)
y_train = data_train['koi_disposition'].apply(lambda val: 1 if val == 'CONFIRMED' else 0)
X_train = data_train.drop(columns=['koi_disposition', 'kepid', 'kepoi_name'])
y_test = data_test['koi_disposition'].apply(lambda val: 1 if val == 'CONFIRMED' else 0)
X_test = data_test.drop(columns=['koi_disposition', 'kepid', 'kepoi_name'])
print('X_train')
display(X_train.head())
print('y_train')
display(y_train.head())

# Scale
scaler = MinMaxScaler()
X_train = scaler.fit_transform(X_train)
X_train = pd.DataFrame(X_train, columns=scaler.get_feature_names_out())
X_test = scaler.transform(X_test)
X_test = pd.DataFrame(X_test, columns=scaler.get_feature_names_out())

# Build model
def build_model(hyperparams, n_timesteps=30, n_time_features=2, n_context_features=3):

    dropout = random.choice(hyperparams['dropout'])
    learning_rate = random.choice(hyperparams['learning_rate'])
    time_layer = random.choice(hyperparams['time_layer'])
    batch_size = random.choice(hyperparams['batch_size'])
    n_neurons_in_order = []

    # Input
    time_input = Input(shape=(n_timesteps, n_time_features), name='time_input')
    context_input = Input(shape=(n_context_features,), name='context_input')

    # RNN
    if time_layer == 'LSTM':
        n_neurons = random.choice(hyperparams['n_neurons'])
        n_neurons_in_order.append(n_neurons)
        time_out = LSTM(n_neurons, return_sequences=True, name='time_1')(time_input)
        time_out = Dropout(dropout, name='time_drop1')(time_out)
        time_out = BatchNormalization(name='time_batch_norm1')(time_out)
        n_neurons = random.choice(hyperparams['n_neurons'])
        n_neurons_in_order.append(n_neurons)
        time_out = LSTM(n_neurons, return_sequences=False, name='time_2')(time_out)
        time_out = Dropout(dropout, name='time_drop2')(time_out)
        time_out = BatchNormalization(name='time_batch_norm2')(time_out)
    elif time_layer == 'GRU':
        n_neurons = random.choice(hyperparams['n_neurons'])
        n_neurons_in_order.append(n_neurons)
        time_out = GRU(n_neurons, return_sequences=True, name='time_1')(time_input)
        time_out = Dropout(dropout, name='time_drop1')(time_out)
        time_out = BatchNormalization(name='time_batch_norm1')(time_out)
        n_neurons = random.choice(hyperparams['n_neurons'])
        n_neurons_in_order.append(n_neurons)
        time_out = GRU(n_neurons, return_sequences=False, name='time_2')(time_out)
        time_out = Dropout(dropout, name='time_drop2')(time_out)
        time_out = BatchNormalization(name='time_batch_norm2')(time_out)
    else:
        print('Unknown option given to "time_layer" hyperparameter! Use "LSTM" and/or "GRU".')
        return None

    # Context
    n_neurons = random.choice(hyperparams['n_neurons'])
    n_neurons_in_order.append(n_neurons)
    context_out = Dense(n_neurons, activation='relu', name='context_out')(context_input)

    # Combined
    combined = Concatenate(name='combined')([time_out, context_out])
    n_neurons = random.choice(hyperparams['n_neurons'])
    n_neurons_in_order.append(n_neurons)
    combined = Dense(n_neurons, activation='relu', name='combined_dense1')(combined)
    combined = Dropout(dropout, name='combined_drop1')(combined)
    combined = BatchNormalization(name='combined_batch_norm1')(combined)
    n_neurons = random.choice(hyperparams['n_neurons'])
    n_neurons_in_order.append(n_neurons)
    combined = Dense(n_neurons, activation='relu', name='combined_dense2')(combined)
    combined = Dropout(dropout, name='combined_drop2')(combined)
    combined = BatchNormalization(name='combined_batch_norm2')(combined)

    # Output
    output = Dense(1, activation='sigmoid', name='output')(combined)
    model = Model(inputs=[time_input, context_input], outputs=output)

    # Return model
    optimizer=Adam(learning_rate=learning_rate)
    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['precision', 'recall', 'accuracy'])
    return model, {'dropout': dropout, 'learning_rate': learning_rate, 'time_layer': time_layer, 'batch_size': batch_size, 'n_neurons_in_order': n_neurons_in_order}

# Format X_train and y_train into numpy arrays
time_cols = []
flux_cols = []

for col in X_train.columns:
    if (col.find('TIME') != -1):
        time_cols.append(col)
    elif (col.find('PDCSAP_FLUX') != -1):
        flux_cols.append(col)

X_train_time = X_train[time_cols].to_numpy()
X_train_flux = X_train[flux_cols].to_numpy()
X_train_timesteps = np.stack((X_train_time, X_train_flux), axis=-1)
X_train_context = X_train[['koi_ror', 'koi_dikco_msky', 'koi_max_mult_ev']].to_numpy()
y_train = y_train.to_numpy()

# Tune
hyperparams = {'learning_rate': [1e-3, 1e-4, 1e-5], 'n_neurons': [16, 32, 64, 128, 256], 'dropout': [0, 0.1, 0.2, 0.3, 0.4, 0.5], 'time_layer': ['LSTM', 'GRU'], 'batch_size': [16, 32, 64, 128]}
num_fits = 50
precisions = []
models = []

for i in range(num_fits):

    print(f'Fit {i+1} out of {num_fits}')

    model, chosen_params = build_model(hyperparams)

    early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)
    model.fit([X_train_timesteps, X_train_context], y_train, epochs=200, batch_size=chosen_params['batch_size'], validation_split=0.2, verbose=1, callbacks=[early_stopping])
    precision = model.history.history['val_precision'][-1]
    precisions.append(precision)
    models.append((model, chosen_params))

# Best model
best_index = precisions.index(max(precisions))
model = models[best_index][0]

# Show best hyperparams
print()
print('Best Hyperparameters')
print(models[best_index][1])

# Save model
joblib.dump(model, 'RNN.pkl')
joblib.dump(model.history, 'RNN_history.pkl')
joblib.dump(models[best_index][1], 'RNN_params.pkl')
```

Code Cell:
```python
import matplotlib.pyplot as plt

# Plot validation loss across epochs
plt.plot(range(len(model.history.history['val_loss'])), model.history.history['val_loss'])
plt.xlabel('Epoch')
plt.ylabel('Validation Loss')
plt.title('Validation Loss Across Epochs')
plt.show()
```

Code Cell:
```python
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay

# Show classification report
best_model = model
y_pred = best_model.predict([X_train_timesteps, X_train_context])
y_pred = (y_pred > 0.5).astype(int)
print('Classification Report')
print(classification_report(y_train, y_pred))

# Confusion Matrix
confusion = confusion_matrix(y_train, y_pred)
disp = ConfusionMatrixDisplay(confusion)
print('Confusion Matrix')
disp.plot()
```

Code Cell:
```python
from sklearn.metrics import roc_auc_score, roc_curve
import matplotlib.pyplot as plt

# Plot ROC curve
y_probs = best_model.predict([X_train_timesteps, X_train_context])
fpr, tpr, thresholds = roc_curve(y_train, y_probs)
auc = roc_auc_score(y_train, y_probs)
plt.plot(fpr, tpr, label=f'AUC: {auc:.2f}')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for Training Data')
plt.grid(linestyle='--')
plt.legend()
plt.show()
```

Markdown Cell:
## Markdown Content:
The validation loss is noisy so the best hypertuned model will be trained for more epochs with a larger batch size.

Code Cell:
```python
# Train best model over more epochs

# Build model
def build_final_model(hyperparams, n_timesteps=30, n_time_features=2, n_context_features=3):

    time_layer = hyperparams['time_layer']
    dropout = hyperparams['dropout']
    learning_rate = hyperparams['learning_rate']
    batch_size = hyperparams['batch_size']
    n_neurons_in_order = hyperparams['n_neurons_in_order']
    layer_index = 0

    # Input
    time_input = Input(shape=(n_timesteps, n_time_features), name='time_input')
    context_input = Input(shape=(n_context_features,), name='context_input')

    # RNN
    if time_layer == 'LSTM':
        n_neurons = n_neurons_in_order[layer_index]
        layer_index += 1
        time_out = LSTM(n_neurons, return_sequences=True, name='time_1')(time_input)
        time_out = Dropout(dropout, name='time_drop1')(time_out)
        time_out = BatchNormalization(name='time_batch_norm1')(time_out)
        n_neurons = n_neurons_in_order[layer_index]
        layer_index += 1
        time_out = LSTM(n_neurons, return_sequences=False, name='time_2')(time_out)
        time_out = Dropout(dropout, name='time_drop2')(time_out)
        time_out = BatchNormalization(name='time_batch_norm2')(time_out)
    elif time_layer == 'GRU':
        n_neurons = n_neurons_in_order[layer_index]
        layer_index += 1
        time_out = GRU(n_neurons, return_sequences=True, name='time_1')(time_input)
        time_out = Dropout(dropout, name='time_drop1')(time_out)
        time_out = BatchNormalization(name='time_batch_norm1')(time_out)
        n_neurons = n_neurons_in_order[layer_index]
        layer_index += 1
        time_out = GRU(n_neurons, return_sequences=False, name='time_2')(time_out)
        time_out = Dropout(dropout, name='time_drop2')(time_out)
        time_out = BatchNormalization(name='time_batch_norm2')(time_out)
    else:
        print('Unknown option given to "time_layer" hyperparameter! Use "LSTM" and/or "GRU".')
        return None

    # Context
    n_neurons = n_neurons_in_order[layer_index]
    layer_index += 1
    context_out = Dense(n_neurons, activation='relu', name='context_out')(context_input)

    # Combined
    combined = Concatenate(name='combined')([time_out, context_out])
    n_neurons = n_neurons_in_order[layer_index]
    layer_index += 1
    combined = Dense(n_neurons, activation='relu', name='combined_dense1')(combined)
    combined = Dropout(dropout, name='combined_drop1')(combined)
    combined = BatchNormalization(name='combined_batch_norm1')(combined)
    n_neurons = n_neurons_in_order[layer_index]
    layer_index += 1
    combined = Dense(n_neurons, activation='relu', name='combined_dense2')(combined)
    combined = Dropout(dropout, name='combined_drop2')(combined)
    combined = BatchNormalization(name='combined_batch_norm2')(combined)

    # Output
    output = Dense(1, activation='sigmoid', name='output')(combined)
    model = Model(inputs=[time_input, context_input], outputs=output)

    # Return model
    optimizer=Adam(learning_rate=learning_rate)
    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['precision', 'recall', 'accuracy'])
    return model, {'dropout': dropout, 'learning_rate': learning_rate, 'time_layer': time_layer, 'batch_size': batch_size, 'n_neurons_in_order': n_neurons_in_order}

# Tune
hyperparams = {
    'dropout': 0.1,
    'learning_rate': 0.0001,
    'time_layer': 'LSTM',
    'batch_size': 32,
    'n_neurons_in_order': [256, 128, 64, 128, 16]
    }
model, chosen_params = build_final_model(hyperparams)
#early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)
model.fit([X_train_timesteps, X_train_context], y_train, epochs=200, batch_size=chosen_params['batch_size'], validation_split=0.2, verbose=1)

print(f'Final Hyperparameters: {chosen_params}')

# Save model
joblib.dump(model, 'RNN.pkl')
joblib.dump(model.history, 'RNN_history.pkl')
joblib.dump(chosen_params, 'RNN_params.pkl')
```

Code Cell:
```python
import matplotlib.pyplot as plt

# Plot validation loss across epochs
plt.plot(range(len(joblib.load('RNN_history.pkl').history['val_loss'])), joblib.load('RNN_history.pkl').history['val_loss'])

plt.xlabel('Epoch')
plt.ylabel('Validation Loss')
plt.title('Validation Loss Across Epochs')
plt.show()

# Plot validation precision across epochs
plt.plot(range(len(joblib.load('RNN_history.pkl').history['val_precision'])), joblib.load('RNN_history.pkl').history['val_precision'])
plt.show()
```

Code Cell:
```python
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay

# Show classification report
best_model = model
y_pred = best_model.predict([X_train_timesteps, X_train_context])
y_pred = (y_pred > 0.5).astype(int)
print('Classification Report')
print(classification_report(y_train, y_pred))

# Confusion Matrix
confusion = confusion_matrix(y_train, y_pred)
disp = ConfusionMatrixDisplay(confusion)
print('Confusion Matrix')
disp.plot()
```

Code Cell:
```python
from sklearn.metrics import roc_auc_score, roc_curve
import matplotlib.pyplot as plt

# Plot ROC curve
y_probs = best_model.predict([X_train_timesteps, X_train_context])
fpr, tpr, thresholds = roc_curve(y_train, y_probs)
auc = roc_auc_score(y_train, y_probs)
plt.plot(fpr, tpr, label=f'AUC: {auc:.2f}')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for Training Data')
plt.grid(linestyle='--')
plt.legend()
plt.show()
```

Code Cell:
```python
import joblib
import matplotlib.pyplot as plt
import numpy as np

# Plot both validation and training loss across epochs
val_loss = joblib.load('RNN_history.pkl').history['val_loss']
train_loss = joblib.load('RNN_history.pkl').history['loss']
epochs = range(len(val_loss))

plt.plot(epochs, val_loss, color='orange', label='Validation')
plt.plot(epochs, train_loss, color='blue', label='Training')
plt.title('Loss Across Epochs')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.yticks(ticks=np.arange(0.2, 0.9, 0.1))
plt.grid(linestyle='--')
plt.legend()
plt.show()
```

